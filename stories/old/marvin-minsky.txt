marvin.minsky

Marvin Minsky (1927-2016) was one of the pioneers of the field of Artificial Intelligence, founding the MIT AI lab in 1970. He also made many contributions to the fields of mathematics, cognitive psychology, robotics, optics and computational linguistics. Since the 1950s, he had been attempting to define and explain human cognition, the ideas of which can be found in his two books, The Emotion Machine and The Society of Mind. His many inventions include the first confocal scanning microscope, the first neural network simulator (SNARC) and the first LOGO 'turtle'.

When the camera of the future gets a couple of good pictures of your face, then it'll just keep putting them in there instead of bothering with reality, because it will track you around. In fact one of Nicholas Negroponte’s students in the... it must have been in the early 1960s, believe it or not, made something called the Zero Bandwidth Videophone. This was a telephone that... because they didn’t send pictures over telephones yet, although Bell labs had made... made a couple of videophones, but they were very slow and low resolution, so the Zero Bandwidth Videophone was very cute; you took about 50 pictures of a friend of yours and stored them in the telephone, and then these hackers had written a program which could guess what your expression was from the frequencies in your vowels, or something.
[Q] Yea, and put up the appropriate…
Right, so if you were… if you were scowling it had a fairly good chance of putting up your scowling face, or if you were smiling… it was just, you know, just a little better than chance, but it was very funny, and…
[Q] When was that?
It was before… before the ARPANET and things like that, which would let you actually send pictures.

The things I remember, of course, are not things I remember because I remember remembering them, and nobody's been able to trace very reliable memories of children earlier than… about two or three years of age because when you try to verify these recollections they're often wrong, or if you interview the person, the person remembers remembering them from time to time, and so forth.  So I have some unreliable memories of playing in a sandbox a lot when my mother was a student of something – social worker – whatever, at Vassar College.
But this is a synthetic image, I think, because when I think about her I remember putting sand in a pail and inverting it, and that could have happened anytime in early childhood.  And so I don't think I have any good memories; I can barely remember a few events in grade school… and I remember a few things about the home.  My father was a surgeon – eye surgeon, ophthalmology – but he played piano a lot and he could tie knots with one hand.  And I asked him why he played piano so much, and he once said that it improves one's finger dexterity, and if you're an eye surgeon you have to be very good at manipulating small things fairly quickly and... that's all I remember about that, except that we had a player piano, and I used to imitate things that my father did, like making up little Bach sounding études or fugues or pieces, and it seemed to me that this was a natural thing to do.  And I remember much later talking to other children at school and discovering that they didn't usually have music in their head and they actually had to have some phonograph or something to play music in order to hear it, which seemed odd to me.
So my memory of early years is just full of little snapshots of taking things apart and walking with friends and… but nothing coherent, and I can't reconstruct the exact layout of rooms in… in early homes or in schools, the world was mostly homes and schools, and a few outdoorses… and the important things were talking about ideas with friends, I think. So that's my childhood. What can we do?

I went to the Fieldston School from, I think, fourth grade on.  We… we were, we lived in New York City, downtown – 86th Street – in the first… my first few years.  And apparently I had a high IQ, and so for one year or half a year I was in a special school called PS 500, which was for more precocious children.  And then we went… moved to Riverdale in New York, and I was in a public school for either a half a year or a year, and… I don’t remember that very clearly, except occasionally there were bullies.  And then for the next five years or so I was in Fieldston School, which was a wonderful place, and I… I didn’t feel particularly exceptional at any time because I had four or five friends who were… could understand everything I was trying to understand and maybe more, maybe less, but it was a community of four or five friends who were all on the same level.

I remember one day some teacher was talking about me when I was… they didn’t know I was in the room, and one of them said, maybe he’s another J Robert Oppenheimer.  So that seemed very strange to me because I’d never heard of a person whose first name was a single letter, and I didn’t know what that meant for some years later.  But… but he had been a student at Fieldston School a few years before, and of course when I was at graduate school at Princeton I met him later and he took me to lunch with Einstein and Gödel and visitors like that, von Neumann... people from time to time.

But actually I always felt rather slow, because if some mathematical thing came up it took me an hour or two to understand it, whereas in most things in real life, you understand something right away or you don’t at all.  And even when I was a graduate student I was focusing on a part of mathematics called topology at first, and I would read a page in a topology book and it would take me an hour or two to understand it; and my friend Herb Forrester – another mathematician about the same age – and I'd ask him what this means, and he would look at it right away.
Just a few years ago I was trying to understand some physics paper, and I… I was being very slow about it and I couldn’t figure it out, and Steve Wolfram was around, so I said… showed him the paper and I said: 'Can you tell me how this works?'  And Steve looks at it for a couple of minutes and he says: 'Oh, it’s really quite simple, it’s this and that and that' and I said: 'How did you understand that so quickly?'  And Wolfram said: 'Oh, I have a special method; if you give me a paper on physics I never look at the words; I just look at the equations and figure out what they ought to mean.'  And I thought that was nice and I practiced doing it myself for a few hours, and I discovered generally it’s much faster, and if… if you don’t get it at all, there’s probably some real reason why not, but... anyway, so I never felt particularly bright, because in all these years at Fieldston I was with other very smart children.  And then after that I moved to the [Bronx] High School of Science, and that was full of other friends who were at least as fast as I was.  They elected me president of the Math Club one day, and I thought that was very strange because it took me so long to understand a page of mathematics even then, but apparently I was good at using it once I got it.  And then my parents must have been afraid that I wouldn’t get into a good college, so for my senior year they sent me to Andover, which was also full of pretty advanced children, but… but except for a couple of them, they seemed rather retarded to me, because I had spent the last 11 years in pretty advanced schools.

I mean, all this is about the question of whether I ever felt particularly intelligent, and once I got to Harvard, I had somehow selected a group of four or five friends who became close friends for many years, and they were terribly smart. But generally when I wandered around the University most of the students seemed pretty mediocre; and after a long time I realized that a lot of them were good at something else, like social organizations or… or other maybe intellectual but not technical subjects, and it was rather disappointing.
But I was very lucky, and I quickly found myself with some friends who were fantastically advanced.  A young mathematician named Andrew Gleason who had come in first in a national mathematics competition three years in a row – that had never happened before – and he knew a vast amount of mathematics; and every time something came up he… we had lunch very frequently, and I’d ask him some question and he would tell me, that’s a whole field and here’s how it works and... I also ran into a couple of wonderful young psychologists, Joe Licklider and George Miller who were pioneers in cognitive psychology and… some biologists and… I just found myself in an atmosphere where my friends were young professors rather than colleagues and students.  And it’s always been like that, except that when I became a professor, by some collection of miracles at MIT, I attracted a really quite large population of… of exceptional inventors and… as computer science was just being developed, the kind of young people who are often called hackers – which is a term that has two meanings – one is, hobbyists who are very good at exploring new concepts of computation before anyone else, and then there’s the sort of safecracker hacker type who’s somebody who tries to penetrate computer systems and break them and… unfortunately the same word was used for both communities. And the people in both communities were somewhat similar, because to… to unpack a complicated system and understand it and find its weakest parts and break them takes the same kind of skill it takes, at least in many cases, to invent such systems.  So maybe it’s not an accident that the same word was used for both the constructive and destructive elements of that society.
Anyway, when I finished graduate school and… moved into the university as a teacher, it was sort of a continuous development, and still having very advanced older friends and having very advanced younger friends… so, it was heaven.  It was just any… it was being in an environment where everything seemed possible, and if I had an idea, I could either try to do it myself, which I might be able to, or I’d mention it to one of these kids, and three days later a system would appear that today would take a couple of years if some commercial company tried to do it.

She was very concerned with social things and always worried about appearances and manners and… my father wasn’t present so much because he was busy get… he would get up at four or five o’clock and go and do surgery or go to his office and see 40 or 50 patients a day and come back and… we spent some time together, but he was usually exhausted and... he had become Chief of Ophthalmology at Mount Sinai Hospital, which is a very responsible position, and was always in demand for the latest techniques in eye surgery.
So he supported everything I did, and if I needed some new tool or equipment or something he'd… he would know somebody who knew how to get it, so I had a lot of very supportive adult friends.  We had a pediatrician, Michael J Karsch, who lived nearby, and of course when I was a child these were the days when if you were sick, the doctor was likely to come to your house, or you could go to the office, neighborhood things; and if I had some complicated… I got a lot of electronic devices when I was a child because there was – this is just post war, talking about 19… after 1941 or so – there was lots of electronic equipment because partly… mainly because of the war, there was tremendous progress in electronics; electronics was big and easy to understand because everything was made of vacuum tubes and big parts that… there were no integrated circuits or black boxes, but occasionally there was a… some kind of box filled with tar that you couldn’t understand. And if I had some gadget I couldn’t understand then I would toddle over to Mick Karsch’s office and we would fluoroscope it. So it’s always seemed to me that one thing that’s missing in modern life is that there should be a fluoroscope in every corner drugstore – I don’t know if there are corner drugstores, even – you know, in my childhood the store on the corner was a candy store or a drugstore, but now drugstores are huge and they wouldn’t fit on a corner, blah, blah!  But no child is allowed to be near a fluoroscope, and so there’s no way to unravel the mystery of sealed boxes.

I don’t think it mattered because almost everyone was Jewish. Having lived in New York, it was…  no, it was never significant; I don’t remember ever being in a… an environment where there were significant anti-Semitic influences. But I know that my family was always aware of this problem and… I think one of the reasons why they moved me from the High School of Science to Andover was to get into the… out of the Jewish mainstream. 'Cause, there was some concern about that, but it never touched me.

I know there were groups of children who were interested in sports and there were children who were interested in literature and… these friends of mine were almost all… well, some of them were interested in music, but that’s a curious thing, that my impression is that most mathematicians are attracted to certain kinds of classical music, complicated things, whereas very few musicians are attracted toward mathematics:  it’s a very unsymmetrical thing and… of course, one generous theory is that the musicians don’t need mathematics because they’re already dealing with very complicated abstract structures very well, and maybe in many respects it’s the same… it uses the same parts of brains, but I don’t, I’ve never seen any attempts to study this.
But I seems to me, most of the things I can remember discussing with friends were… were technical things and inventions and ideas about how to make things that did things, rather than…  I don’t remember ever discussing social matters very much; maybe we were all a little bit autistic.

It’s interesting… it… I can’t remember, or my representation of early years is, there’s home, and things one did there, and there’s school, and there’s no third world. There’s the world… well hobbies, and I was always building things, and maybe with blocks as a child… I remember a few strange scenes. For example, an image that I have repeated many times is building a tower out of Tinker toys; Tinker toys are these wonderful objects made of little round spools with holes in them and little round sticks, and you make things by putting the sticks in the holes.
And it’s sort of the opposite of Lego, because if you… Lego became popular, but it’s bricks, and you could only have right angles, and to make something strong you have to just pile on a lot of things, because with Lego blocks there’s really no way to make a triangle.  With Tinker toy, it’s almost the opposite, because Tinker toy spools and sticks are kind of optimal, because you can make triangles with three sticks and three spools, and a triangle is terribly strong: the reason is it can’t change its shape without changing the length of the stick. If you make a square, then… then it can do this; it wobbles. I often wonder how many modern children don’t understand the strength of structures because all the toys they grew up with were blocks and Legos… Legos plugged together, and you cannot make a triangle.  So it’s a strange world, and we could trace the decline of American inventiveness perhaps to… to this revolution.
The other toy was Erector sets, or the English Meccano set, which was almost the same but higher quality. And I was addicted to those, and in fact even today, if I need to make something quickly, I’ll look for my Meccano set – Number Ten Meccano set – but it’s not home, it’s at my son Henry’s house, so that his… my grandchildren can play with it.
But that was the world that I lived in. If I wanted to build something, I could make something quickly out of these construction sets.  There's a… there was another Swedish one called FAC, which was more like Tinker toy, and it had clamps and rods. And between the Erector and Meccano sets which allowed you to build structures with beams and cross bracers, and the FAC sets and so forth, where… where you could make pulleys and gears work very well, with that kind of equipment a child could build engineering prototypes of… of rather complicated things which are almost impossible today for a child to build because all the child has is Lego, which makes it very hard to get gears to… and pulleys to work.  So it’s a… it's a curious thing that the toys became more sophisticated, but more realistic and less flexible.

I built a robot arm, but I can’t remember if I was in college by then; that must have been later, don’t you think? When did I build that? I was already at MIT. Oh, yeah, I built some structure out of Tinker toy in the lobby of a hotel; I had a lot of Tinker Toys – sticks and rods – and my impression is that this… this tower was about 15ft high, much higher than three times my height, and adults would come by and express some question about how… how could such a small person build such a high object?  And I was having trouble understanding how they could wonder such a thing, because the thing isn’t very heavy, so it’s just a matter of building another layer and then lifting it up carefully and putting it on, and…
And one of the things that I understood rather early was that the longer something is, the easier it is to balance it. And one day when I was much older we went to the circus in Boston, watching a clown who was doing some juggling, and also a wonderful trick where you spin a plate; and there must be a dent in the bottom of this plate.  So he puts the plate on a stick and it’s still spinning, and he takes another stick, like a quiver of arrows, and puts it on, and it gets higher and he’s walking around balancing it, and he keeps putting more sticks…  And it’s really quite remarkable, because he has enough sticks that this spinning plate gets up to the top floor of the Boston Garden which is really six or seven stories high, so it must be 80ft of… 70 or 80ft, which means maybe 20 of these sticks.
And the audience gets more and more excited as this clown is balancing this enormously long structure, and I’m staring at this audience and realizing that here are 10,000 people who don’t understand that the longer is the stick, the easier it is to balance. What I couldn’t figure out is why the plate is still spinning, which I still don’t know; it must have very low friction. But again, this is a picture where you see a mass audience who don’t seem to understand what they’re seeing at all, in terms of elementary physics. If you have an 80ft stick, and you tilt it, it falls very, very slowly, and you can walk this fast and balance it, and if you go too far, then you go back; if it’s this high, it falls over before you can move.
And… so from this very early childhood scene to this re-enactment of it in the... in the circus, this realization that people don’t understand the most important and simplest things about physics, how… how does that happen? Everyone knows that you can pull something with a string but you can’t push it, so there’s some elementary facts about things that everybody understands. And then there seem to be equally elementary things that somehow are not part of education, and people don’t often learn it by experience, for some reason.

And there’s a wonderful thing about the bicycle, which is that if you try to control it then you fall off, but if you take your hands off and just sit there, then…  so learning to ride a bicycle is, maybe, the pinnacle of negative learning: leave the thing alone and you’ll be all right; try to control it and you’ll fall over. Anyway, here’s a war surplus gyroscope, which I believe is from an Atlas missile. There was a wonderful war surplus… military surplus depot in Massachusetts until the… until about 19… late 1980s, when it burned down, maybe mid 1980s; and we could get parts of… one of my friends assembled almost a complete ballist… intercontinental ballistic missile system with the Atlas rocket engine and empty warhead and guidance system, and he assembled it in the backyard of a home that Buckminster Fuller had in this neighborhood. I wonder what happened to it? I’ll ask him.

I finished school in 1944 – spring – and… and at that time of course World War II was still running and I would have been drafted into the military, but the news came around that there was a… arrangement that you could volunteer to enlist in the Navy and they would guarantee you a year of training in electronics. The Navy was short of radar and radio specialists and all that sort of thing, and they had a school in Chicago for training people in this field. So in fact I enlisted in the Navy because I was almost certain to be drafted otherwise; nothing else wrong with me.

And I was in boot camp when Japan surrendered, so I had been in the Navy for just a few months… in… in the Chicago area. And strangely enough, there were four or five… this was a company of about 200 recruits, most of whom were rather relatively ignorant people from the middle of the United States, and I remember being astonished at meeting people who didn’t know how to swim.

There were also about 10 people in this company who were very advanced in fields like mathematics, and they had exercised the same option I had of joining this… passing a test to get into this program for radio training school. And like me, a lot of them… several of them already knew a lot of electronics anyway, and there were a couple of mathematicians from whom I learned a great deal of mathematics.  John Wermer became a professor at Yale, and… what’s his name? Parker was a… became a professor at Pomona College, and so that was almost like going to college.  These… Herbert Forrester, another; three fairly distinguished mathematicians in this group.
And then they dropped this atomic bomb and… we continued to hang around this boot camp, but there was nothing to do, and the… I guess the government had decided it couldn’t send all the soldiers back into civilian life until the jobs appeared, and so it was a strange year. But during this year I… instead of doing con… conventional military things I was learning more mathematics and electronics, and I had a part-time job making ice-cream sodas in the base camp and… it was surrealistic and bizarre to have such an experience.

I don’t think it was an important part then, except that since people were going to Japan and getting killed in this period it was a great relief. And for several days after the news came out I was convinced that this was actually a… a hoax, because Hiroshima was a port city, and so I assumed that in fact somebody had equipped a barge with a few thousand tons of TNT, just as the cliché went, and they’d slipped this barge into the harbor, and then they flew a little airplane over and dropped something and set off this bomb. And so I assumed this was a great trick for fooling the Japanese into thinking we had an atomic bomb.
But Nagasaki wasn’t so accessible, so… I think many people must have wondered, why… why did we drop two bombs? Why wasn’t one enough? My first thought was that it was to convince them that it wasn’t a hoax. A few years later then, I met Oppenheimer for the first time when I was in graduate school at Princeton, and he was very hospitable… and tragic.

Well, it was an interesting thing… I don’t think I ever realized while in high school that people were still inventing mathematics; that really… I must have heard something about it, but some of us knew a little calculus, and certainly I had a full course in calculus at Andover in that fourth year of high school, but I knew about it when I was at the High School of Science, and… but that was 400 years old or… when was Newton? 1630, or… so, I didn’t have a sense that there was still mathematics happening.
Nowadays when I go to my computer every now and then, I ask Google to search for John Baez, who I believe is a child of Joan Baez, and he’s a mathematical physicist and runs a page called This Week In Physics. And if you look at this week in – but it covers a lot of mathematics – and you go to this website and you see 200 new discoveries in various fields of… related to mathematics and physics. And it wasn’t till I got to Harvard and started having lunch with Andrew Gleason, who was… who had been one of the great code-breakers of the secret American cryptography community in… during World War II which was just a couple of years before that… and we used to talk about new discoveries in various parts of mathematics almost every day.
So in… in all of my childhood and high school, I don’t think I ever had the sense that mathematics was a profession you could enter and start working right away on countless interesting unsolved problems, some easy, and lots of easy problems left to solve, and certainly lots of hard ones. And when I went to grad… so I did a lot of mathematics in college, but mostly learning it, inventing a few things, and when I went to Princeton four years later I started to invent some new kinds of mathematics, and ever since I… it’s hard to believe that in… still in the year 2010 and 2011 there are a lot of simple looking mathematical problems that are still unsolved, and lots of moderately hard looking ones that are presumably not so hard to solve. If a simple problem has hung around for 300 years and nobody’s made any progress, then it’s not simple. Or… so the easiest thing is to work on one of the harder problems where… where there’s a chance of making a little progress.  But anyway, it’s interesting that I didn’t know that… that any such things were possible all of… all of my early life.

What did I think I was going to do in my career? I have an oversimplified story about how I viewed the future when I was in college, and the simplified view was that the first thing I thought is that… maybe I would do electrical engineering. But I ran into… quite quickly met some people who were very good at that, so it seemed to me, well, they didn’t need me. And then I thought that, well, I’m pretty good at mathematics, but after spending those... year of lunches with… with Andrew Gleason I realized that, well, there’s some mathematicians who are so much deeper and faster than I am that if I really went into that I wouldn’t make a big difference, because Andy could do something in a few hours that might take me a few months.
So the second best mathematician is really a waste of resources, so I didn’t really see a career in mathematics even though I was… except for the people who were better than me, I was very good at it. And then I got interested in… or at the same time I became… I started to invent learning machines and theories of neural networks, and that’s another story: that all came from opening a book called Mathematical Biophysics.
Early in my college career I was just wandering through a library, and I came across this book by Nicholas Rashevsky, and it was full of little chapters like how does a cell divide, or how does a neuron conduct a signal, or just very simple questions: how does… how does a heart pump? And each little chapter takes some commonsensical question about some aspect of biology and makes a mathematical theory, and usually this mathematical theory is different from any other theory. For this theory of how the heart pumps, there was a little paper by Oliver Selfridge, or maybe it was by Oliver Selfridge and Walter Pitts, and it described how certain waves would happen in a sheathe of muscle, and the interesting property of these waves is that the faster ones would drive the slower ones out, and so the whole thing would eventually beat as a single unit, because it was being controlled by the fastest source of pulses, or something like that.
So this was using the theory of topology, an obscure branch of mathematics, to understand a biological thing. I was really fascinated by that, so maybe I was going to be a biologist. But other people were pretty good at that too, so why bother?

And then when I got interested in making learning machines, I had great fortune to become friends with two young incredibly original professors, George Miller and Joe Licklider, and they were making up new kinds of theories about cognitive psychology. And at that time, in the late 1940s, they were way ahead of everyone else; it was sort of the time that Norbert Wiener’s Cybernetics was beginning to spread, and they understood those theories, and I understood them even better, and so that sort of made my decision for me. That is, here was this new field called cognitive psychology, and I was really good at it, getting new ideas, and except for a few other people like Sigmund Freud and William James, who maybe had better ideas than I did, there wasn’t much competition. Licklider and Miller were… accepted me as an equal, and so you could say that my career was based on cowardice, or… or something. If there was a field where there were five or 10 people who were better at it, why bother? Because in these fields, you know, if you were a very good physicist and you ran across a Richard Feynman, what’s the use?
And I was a… quite a good physicist, actually, but I had the fortune to run across Richard Feynman, again we became friends, and I wrote a… in fact I wrote a nice paper that he liked enough to publish, but… but I couldn’t see being serious about it, because with people like Murray Gell-Mann and Alex Rich, and people like that.

The days at Harvard… there was something about that period that might never be repeated, which is to be a young scientist maturing just at the end of World War II, when something had happened that very few people knew about, namely, there had been a collection of refugees. So that when in 1953 I was finishing my PhD thesis in the math department at Princeton – this was after college – this was on neural networks and theories that I had developed after seeing Rashevsky’s book with the earliest simple theories of neural networks, so I was in the math department and they said: 'Well, is this mathematics?' Well, the answer is, it’s easy to find out, let’s send the draft of the thesis to John von Neumann, who’s at the Institute in Princeton. And he sends a message back, saying: 'Well if this isn’t mathematics now, it soon will be'.  That’s what the head of the department told me.
And this was because people like von Neumann and Gödel and everybody – maybe not Gödel – but many of the people that I knew had been refugees from Europe, and people in the… particularly in the Office of Naval Research, but other agencies during World War II had been systematically finding ways to extract great scientists from Europe rather secretly and quietly, and bringing them to places like Harvard and Princeton and the Institute for Advanced Study, and so forth. So when I was coming through college and out of it… I had this opportunity that maybe no one will ever see again, or maybe it’ll happen in China soon, I don’t know.  But if I had a question, I could ask the… I could just walk over and ask the person who knew more about it than… than anyone else; I got used to that.  If I had a question about optics, I would just walk down the hall and ask Ed Purcell. Well, this was later, when… when I’m back at Harvard in the… after… after I was at Princeton, working on these neural network theories, then I got a Fellowship back at Harvard for three years where I could work on anything I wanted to and… in this atmosphere where if some problem came up, probably the world’s best expert on it was down the hall, or nearby.
[Q] Fabulous. 
Of course, it’s fabulous now, because they’re on the internet, but you have to be fairly… you have to have a reputation to get their attention; whereas at that time they were looking for good students, rather than overwhelmed by them.

There had been some theories of what neural networks could… could do, and the answer was that… neural networks are like logic in the sense that you make… you can make little boxes that can compute simple properties of… of input signals and represent them in their output in some way, like one neuron will recognize whether two inputs come at approximately the same time, and say yes, both, and so that corresponds to saying that for certain two statements are true at the same time, something like that.
So there’s a rule… and another kind of neuron will produce an output if either input is excited, and so that's… that’s called an or, and then there’s a not where you’ll have a neuron that will produce outputs unless a signal comes in, and then will turn it off.  And it turns out with those three kinds of things you can build any kind of more complicated machine you can think of.  And so that was sort of known rather early by Russell and Whitehead and people like that around the turn of the… beginning of the 20th century.
So when I’m going to school in the 1950s a lot has been known about logic and simple machines for 50 years, but then in the 1930s and '50s a lot more was learned about machines by Alan Turing who invented the modern theory of computers in the… 1936 was his outstanding paper, and Claude Shannon who invented the… a great theory of the amount of information carried by signals in… published I think just about 1950; he had also invented logical theories before that. And Shannon became a close friend of ours fairly early in 1952 in fact.

I got interested in neural network ideas from reading a few papers in that great book by Nicholas Rashevsky, a book called Mathematical Biophysics, and... I’m trying to remember the year, there was… among other things he had a journal which published this first paper by McCulloch and Pitts about neural networks. Well, it was maybe the second paper on the logic of neural networks, because Sigmund Freud had actually written one in 1894 or '95, in the from of a paper called Project For A Scientific Psychology, and I believe that paper was turned down for publication, so Freud never got it published, and it wasn’t actually published till 1950. So that’s an interesting story which we should look up... get the dates right. But anyway, I started to work on trying to make... take the McCulloch-Pitts theory of how neural networks could simulate any machine, a very general theory, and I was interested in making more particular specialised theory of how to make neural networks that could learn.

I don’t remember clearly when I got interested in psychology, but I’m sure it was related to accidentally discovering this book called Mathematical Biophysics by Nicholas Rashevsky who had published a journal... for many years, and one of the important papers in his journal was the paper by McCulloch and Pitts on logical... how... how logical modules, little ands... devices that computed ands and ors and nots, could compute more complicated functions of stimuli and produce responses and so forth.
So that was a general theory, and general theories are often easier to construct than specific theories, so this said how you could make any machine, but it didn’t give details.  And I got more interested in, how can I make a particular machine that would learn from experience and do some of the things that intelligent animals can do? So I worked out some ideas about how one kind of neural network could generate different possibilities. The simplest idea would be to try things at random, but that’s rather uninteresting in some ways, it’s much harder to experiment with, and another alternative is to systematically generate different possibilities, and then that could be more efficient because you don’t waste time duplicating things, and on the other hand it might be inefficient, because it makes too many meaningless... explores too many meaningless possibilities.

I worked out a lot of theories about how to make a machine that tries to solve problems by generating hypotheses, and then testing them to see if they make correct... correct predictions and so forth, and this big thesis is maybe 300 or 400 pages.  It first introduces a lot of ideas about how neurons could do this kind of logical reasoning, and then some shorter chapters, but more futuristic, about how to make networks that look at a number of streams of data and then try to predict what will happen next, and so forth. So I don’t even remember clearly exactly what’s in this big PhD thesis, but my impression is that because it’s never been published, and I have a copy and there’s a copy at Princeton and maybe a third copy somewhere else, no-one’s ever really looked at it and followed up any of the ideas.

I should mention that there’s one place where I tried to calculate what can be accomplished by loops of neurons that are arranged in circular pathways so that if you put a certain pattern in, it will sort of echo around and I showed that, mathematically, that under some conditions, the information that you originally put into such a loop will be gradually destroyed and the pulses will become equally spaced, and that’s sort of interesting, because I had trouble proving this.
And one day I ran into another graduate student named John Nash, and I said: 'How would you deal with this problem?' And he looked at my description of it and he said: 'Well, why don’t you expand that into a Fourier Series?' And I said: 'Oh, well, let me think about that'. I couldn’t figure out what he meant, but after a couple of hours I figured out what that would mean and then I did it, and I proved this theorem and…  so John Nash actually must have solved a lot of problems for other students around Princeton while he was working on his big theory of optimal strategies for games, but in particular, this was a problem I just casually asked him one day, and he promptly told me how to solve it, although he was too busy to actually solve it himself.

I was interested in that... trying to design a machine that would collect a lot of experience and then make predictions about what was likely to happen next, and so the... this thesis ends up with that, a little bit theory about that... but then I decided that that was really the wrong way... or a wrong way to, think about thinking. And... I think in 1956, or was it 54? I’ll have to look it up again... another friend of mine named Ray Solomonoff started to produce a theory of how to predict... make predictions from experience based on a completely different set of ideas based on probability theory and, more important, based on Alan Turing’s idea about universal machines; and Solomonoff’s idea was that basically the best way to make a prediction is to find the simplest Turing machine that will produce a certain phenomenon.
Now, there’s no systematic way known where... and in fact it may be impossible to find the simplest Turing machine that produces a certain result, but still this is a wonderfully important idea about the foundations of probability and about how one could build very intelligent machines. So after I got this insight from Solomonoff, I stopped working on neural nets entirely because I felt that was a bottom-up approach, and one really couldn’t design a machine to do intelligent things unless one had some sort of top-down theory of what it is that… what kind of behaviour that machine should have.
Once you have a clear idea of what it is to be a smart machine, then you can start to figure out, well then, what kind of machinery could support that, and what kind of machinery would it take to do that? And then at the end of a rather long path we could ask, and how could we use neurons or things like we find... like the cells we find in the brain, to produce these functions?

Now, right now, most of the world says: if we understood what neurons do enough, well enough, then we could figure out how thinking works. What I’m saying is, it’s really not so important what the neurons do because they probably... they probably have been rearranged by evolution so that small groups of them, like the columns of 400 or 500 neurons that Mountcastle discovered in the 1950s. What’s probably important is to know what these groups of columns of neurons do, rather than how the neurons themselves work, and to guess what’s important about what they do is very hard unless you have a higher level psychological theory of what kinds of knowledge is needed, and how that knowledge is represented, and how do you acquire it, and how do you decide when to suppress one kind of reasoning and use another, and so forth. So all of this happened to me around 1956 or '57, when I started to say, instead of figuring out how to make a thinking machine from little atoms or cells or something, let’s start from the top level the way Freud did and say: What are the highest functions that we want the machine to have? And what sub-machines would it take to support those and that’s worked out?
And this would be a nice way to divide up the field, because there are several hundred thousand people working on neuroscience in the world, a lot of smart people, and there are only about 20 or less on the whole planet who are thinking about: What are the top levels of cognitive operations that... that thinking might be composed of? So what I would like to see is more support and more people working on these high-level theories, and maybe... a few less of this huge number of people working on theories of what neurons do, and then we would be more likely sooner to meet in the middle, and have a complete theory of how to build a smart machine.
So that’s where my thesis ended, about 1954, and I was converted to saying, let’s try to develop higher-level symbolic reasoning systems.  And I wasn’t quite the first to do that, because at Carnegie Mellon Institute... because at Carnegie Mellon Institute there was a pair of... or three researchers, Allen Newell, Herbert Simon and Clifford Shaw, who were working on that very idea of saying: What are the highest level cognitive operations to make a machine that can do reasoning? And for several years I sort of developed my theories in parallel, and adopting some of the ideas that... of Newell, Shaw and Simon. And I had the good fortune that some administrators at the RAND Corporation in Santa Monica were interested in... in this new approach to things, and they had already... they were already supporting Newell and Simon, and they had a... one of the first computers – and we’re talking 1954 or so, there weren’t many computers yet – and so I used to go out there in... in the summers and work on these theories myself, and with mostly Newell and Simon; and that was a very small community.

Actually, I first encountered computers in their most primitive form by hanging around the computation centre – I don’t remember what it was called, it may have been called the computation centre – at Harvard. There was a professor, Howard Aiken, who had developed a relay computer called the Mark I, and it was followed by another one called the Mark II. This was a very large machine entirely made of relays, which are magnetically controlled switches, and this could add numbers and multiply them with all sorts of little clicking switching and so forth. It didn’t have any… to programme it, you took a huge board with wires that you could plug in and connect this to that and this to that, and so forth, so a computer programme then was a board weighing about 50lbs with a few hundred wires plugged into various jacks, and it amounted to what might be the equivalent of 20 or 30 instructions on a modern computer.
So I didn’t think much of that, and it could compute about five or 10 operations per second, which also wasn’t very impressive, so that was a world that I sort of observed,  in fact I went to Aiken’s lectures and talked to the people there, and thought, well, maybe someday this will be fast enough to simulate the kind of things we’re talking about.

He was in charge of a computer called SEAC, which was the Bureau of Standards Eastern Automatic Computer. I forget who had designed this... this machine, but this machine had an actual dynamic memory; mercury delay lines in fact... put a pulse of sound into a pipe and it would take a fraction of a second to go around this long liquid path of mercury, and you could put maybe 500 zeroes or ones into that thing, so it... it was moderately fast. And there were four of these machines made for different places, and that’s the first computer I ever programmed, because – why am I having trouble remembering his name? – Russell Kirsch! Because Kirsch sat me down at a desk and said I couldn’t get up till I’d written a program for the SEAC. So I wrote a program which actually recognised the shape of a couple of graphical letters, and Russell Kirsch in fact wrote some of the very first graphic programs for any computer, including a little picture of a person’s face. So I didn’t actually touch a computer again till maybe 1956 when there were at last some real computers, and I wrote a couple of programs for the one out at RAND.

Our friend Fredkin, who encouraged a digital equipment company to give him a PDP-1... that was the first... that became the first time-shared computer, and that changed the whole world, because instead of punching a deck of cards and waiting till tomorrow to see what happened... Fredkin and McCarthy and a few other people around here connected typewriters up to real computers in real time, and that produced the hacker community who liked to do something and see the result right away.  And there was also a similar community in England - I don’t know much about them, but it was pretty much the same year that – I think it was in Cambridge, but I don't…

There wasn’t such a community… I was at Princeton from 1951 to '54, and I was part of a community that indeed met every day, pretty much, because in the math department at Princeton there was a common room, and the total number of students was rather small; they admitted maybe six or seven per year, and people would stay four or five years or more, so there was maybe 30 graduate students and 10 professors, which was a rather remarkable ratio, and so almost all the students knew almost all the other students and professors, and we almost all were hanging around this common room which was fairly big, and every day, pretty much.
And I would spend most of my time talking to John Nash and Lloyd Shapley and Martin Shubik and Herb Forrester and three or four other mathematicians with different interests, and some of the professors, and when something came up we would talk about it; and then the next day or two, somebody might have made some real progress, or everybody decided it was a waste of time, and... so that was a small community.

I went back to Harvard from '54 to '57 for three years as part of this so-called Society of Fellows, and again that was a little community, but they were... each person was in a different field, pretty much, so it was like a Rotary Club; and there were poets like Donald Hall and John Hollander, two friends, and there was the junior Fellows at the time I was there... what’s his name, the Ant Man? 
[Q] EO Wilson. 
Yep. Ed Wilson was there some of the time – most of the time he would be in a little telephone booth in some jungle watching some animals – and there was Noam Chomsky, who was making his terrible theories about language at the time.

I sort of liked the mathematical part of it, which was largely due to a mathematician named Schützenberger… Marco Polo Schützenberger. But for some reason there had never been any mathematics in linguistics before, and somehow this became immensely popular, and for maybe 30 or 40 years support for semantics and understanding of how language works virtually vanished from the planet earth because of Chomsky’s influence, and most... most universities changed their linguistics departments to grammar departments and theories of formal syntax, and so forth. It’s a phenomenon I hope never to see again, of a... of an important field being replaced by an unimportant one... a wonderful, marvelous phenomenon. In fact Chomsky himself eventually got out of it and went into politics.

I can’t remember much because we were just friends and we talked about many things, but… but since we weren’t really doing much of the same thing it… it never resulted in an accomplishment or a piece of work, or art. But I spent a little more time with John Hollander because he was more interested in… a little more interested in the theory of semantics, but both of them were inspiring poets of very different sorts, because John Hollander is much funnier and… sheer technical cleverness with words, and Donald Hall is much deeper psychologically and…

It was a wonderful environment because it was… well the… the senior… it had about eight senior Fellows, and about 30… 24, I think there were six or eight per year, so… maybe eight, so there were about, somewhere around 30 junior Fellows, and a third of them would change each year because it was a three year Fellowship. But the incredible thing was that it had been set up by President Lowell many years ago, so that they had the key to the University. So one year I had a biology lab, and one year I had a physics lab, and every time I changed my mind about something I could do it, so it was… no… no requirements, no exams, no nothing… no obligations.

My main interest was making… making a new modern theory of psychology, but it switched from neural networks and theories of how the nervous system worked to… to higher level theories of what are the nature of goals and how do you… what is knowledge and how does it help to solve problems, and what kind of search processes are better than others, and what does one have to know to… what about meta-knowledge? What do you have to know about problem solving in general to solve problems in particular?
So there was an interesting step, because the first year I was a junior Fellow, I was still interested in the nervous system, and one of the problems was that you couldn’t buy a diagram of how the neurons were connected or how they worked. And the reason for that was if you take a little piece of brain, like a cubic millimeter, then that’s room for maybe 100 neurons side by side in each dimension, so 100³ is a million. Well, it’s not that bad, but there’s something like 100 billion neurons in the nervous system, and there’s no diagram showing what 1000 neurons do, that didn’t happen till many years later with that worm that Sydney Brenner examined: Caenorhabditis… what’s…?
[Q] C. elegans is it? 
C. elegans, and it has about 300 neurons, and they painfully mapped out where they all went.

Here I am in 1954, and I can’t find a map of the neurons. And if you take a slide of a piece of nervous system and stain it so that all the neurons turn black, then the whole slide is black, and you can’t see anything. So the only information available was from something called the Golgi Stain, which was a fascinating way of treating a piece of nervous system with… I think it was Osmium – some Osmium compound – and it has the property that if there’s a… a leak in a neuron the stain gets into it and the whole neuron turns black, but the other ones around it don’t.  And so this enabled people to get very good portraits of what an individual neuron was, but you could never get any picture of how they were all connected with each other because the thing is so full of dark objects that no light gets through.
So I decided to fix that, and I invented something called the confocal microscope, which was a new way of putting light through a specimen so that if the light bounced off… if you tried to see what was happening at a given point, you only look at light that gets to that point and comes out of it again. And the way to do that is that if some other light bounces into the path of your… that you’re picking up, it gets rejected.  And so I built this gadget which won the Rank Prize – there’s a picture of the Rank Prize up there – and now every biological laboratory has a confocal microscope, and they’re used all over the world. And it was… I wrote a paper on it, and a patent; and I didn’t really have the idea of publishing very clearly in my head then so the patent was issued, but the second confocal microscope was built 20 years later.  So anyway, that was a very exciting adventure, but I sort of showed people how it worked and they said, oh, that’s amazing, but nobody built another one for many years.  That was my fault, I guess, for not getting it published openly.
Where was I? Oh, anyway, but right in the middle of that, when I began to understand Solomonoff’s ideas about making a high level theory of learning and inference I decided to drop all that stuff, and I realized it wouldn’t help to know how the nervous system was wired into you, because it’s so… it's such a mess that until you had an idea of how it worked you wouldn’t be able to decode looking at that diagram, even for a… C.elegans; unless you’d do separate experiments to see what each neuron does, I don’t think anybody could learn much from looking at the diagram. Besides, those neurons aren’t very much connected to each other, anyway.

So it’s right in the middle of the Society of Fellows’ three years that I switched from… from neurology to cognitive psychology. And then when the… that Fellowship is up, I’ve made friends with this wonderful Oliver Selfridge who… whose work I first discovered in the Rashevsky book, and it turned out that he was still around, rather same age as me, and had a laboratory at Lincoln and was working on moderately interesting questions like… at that time most communication was with Morse code, and in order to understand Morse code you had people who had to listen and type, and there was no machine that could transcribe hand-keyed Morse code, because in fact if you looked at the data, some of the dots were longer than some of the dashes. So it was all pretty hard problem, and Selfridge had a group that was doing about five things like that, because Lincoln lab was a military… military electronics laboratory that was mainly concerned with making automatic radar systems to catch Russian missiles going over the North Pole, and that sort of thing.
But his group was doing advanced cybernetics and what was just beginning to be called Artificial Intelligence at that time, and Oliver Selfridge was full of wonderful low level ideas about AI and… a very dramatically inventive guy, and very good at collecting people, so he had this group of wonderful young people and… working on… on radar related things.

He was my model for the laboratory I started with the hackers; that is, I didn’t hire people to do jobs, I hired people who had goals, and… were already inventing… if somebody had invented two or three really new kinds of programs, I’d gobble them up and… my way of hiring people after that was… we had a lot of money from the Office of Naval Research because of Licklider, who had gone to Washington and started this thing called ARPA. So he sent me a lot of money, and if I… so I was… what I was able to do is, if somebody sent a message or a letter – because there was no ARPANET yet – they’d say: 'I’m interested in this', and I’d say: 'Well, why don’t you come here and see how you like working here?' And somebody would come for a week or two. We’d pay them, [sic] enough to live on, and then they’d go away if they didn’t hit it off. I don’t remember ever making a decision to tell someone to go away; it’s really very bizarre, but this was a self-energizing community, these hackers had their own language, they could get things done in three days that would take a month, and if somebody appeared who had the talent, the magic touch, they would fit in.

Most of them originally came from something called the Tech Model Railroad Club, which simulated a model railroad and… these were weird people… like they had an annual contest to see who could… who could in the shortest time ride every New York subway, which takes something like 36 hours or three days, I don’t know; but people would log these things very carefully and would study the schedules and plan their whole trip; and so these people were nuts. And so they were the first ones to use computers because they were automating their railroad; they didn’t have real computers, they made things that worked with relays. And somehow… they still can maintain that activity, but the really smart ones moved into my lab, and that’s where the LISP Language and other things like that started to develop, so all that started in 1958 or '59.

After the Fellowship I spent a year with Selfridge, or maybe two.
[Q] Yes, at Lincoln?
At Lincoln, and then I got a letter from the math department saying: 'Why don’t you be a professor here?' So then I moved to MIT and I taught calculus for a year or two.  And I found that rather tedious, teaching these things, because I would prepare the lectures and then, you know, you had to give two courses every two days, and so I’d spend half the time in between figuring out what I did wrong, and the other half planning the next one and… that went on a couple of years. I had this big group growing and functioning, and then one day I ran into Peter Elias, who was head of the EE department, temporarily. He said: 'How are things going?' And I said: 'Oh, it's… I’m not having enough time to do research'. And he said: 'Well, why don’t you move over to the EE [Department of Electrical Engineering] department, because we have lots of money, unlike the math department, so we only have to teach two courses instead of four'. And then it turned out that for some reason… then I got some money from ARPA – some extra money – and then it turned out I only had to teach two courses every other term. And then one of those disappeared, so now I was teaching one… one course half the year.

I had written this paper in 1957 called Heuris… something… something about heuristics, and that made quite a hit. It was never published, but it was circulated all over the world, and that’s what I worked on, making… improving that theory, and… I think in 1961 I published a huge paper called Steps Toward Artificial Intelligence, which was… I know, the… the early paper was called Heuristic Aspects of the Artificial Intelligence Problem; heuristic means the art of discovery, how do you invent things; and so I had written this paper which was published at Lincoln and circulated, but was never in a journal. Then in 1961 I published… I was almost… it’s almost a book, and that was… Steps Toward Artificial Intelligence. And that… that sort of established the whole field as being pretty serious because it had 300 references, and… and in fact the bibliography was published by Licklider as an appendix to his paper on man-machine interaction; Licklider had turned into a prophet in the meantime.

Well, at first… yes, at first the course I had been teaching diff… teaching calculus and differential equations – this was the first two years of graduate mathematics – and then what I did is I replaced the one of the math courses by an Introduction To Artificial Intelligence, and gradually I was teaching that instead of… and none... none of the math. And then I moved to the EE department because that made more sense.

John McCarthy was a good programmer; I was a… not very good programmer and he had invented this wonderful language called Lisp, which was based on the nice language that Newell and Simon had invented called… no, it’s called IPL, Information Processing Language. IPL is made of little atoms and it’s very tedious, and McCarthy’s invention was… it was more like FORTRAN, which was… which is a language… IPL just had little instructions, FORTRAN has statements, make this true, and Lisp had statements with a very clean syntax. It only had… it basically has five basic verbs, whereas FORTRAN has a big mass of arbitrary… has a big library, and so McCarthy’s was a very elegant cleaning up of… of computer science. And thereafter, there were two kinds of languages, the algebraic language FORTRAN, and this recursive language call… based on Lisp, which… they’re now dying out because of… in a Lisp program, you can write a program that writes Lisp programs so it has a kind of open future; now, no… no-one actually does that yet, but it’s still possible.  In the other languages, it's almost… you can’t write a C program that will write a C program, it’s just… there aren’t any verbs of the right kind, so to me programming hasn’t changed much in 50 years because they got locked into this… strange set of limitations.

'56, that was when I was still a junior Fellow, so there was the… McCarthy organized this meeting on Artificial Intelligence which took place at Dartmouth; he got funding from somewhere, and that’s when we first met Newell and Simon and Selfridge and Solomonoff, and everyone we could find who was working on early Artificial Intelligence. So that was quite an important meeting, in the sense that it created a community that hadn’t existed before and... for example, I… having met Newell and Simon, I... who were partly… working part time at RAND, I started to visit RAND, mostly in the summers, and talking to them and other… so that was another area that had a lot of computer pioneers in sort of one building.

Well, generally, me, McCarthy, Newell and Simon are the… were the best known figures in that, so there’s two pairs of people, which is interesting; there was a little rivalry, and what happened is that they actually had better models than we did for the first year or two. And then, I think mainly because of Simon’s interest, they started to get interested in showing that… that… with experiments on humans… that humans operated in accordance with some of their theories. And so Simon finally produced a big book called Human Problem Solving in which he had some programs to simulate solving certain kinds of problems, and he had tape recordings of people trained to talk aloud while they solved problems and this is a huge book that tries to prove that people work the way that program did.
And no one pays much attention to it anymore, because of course you can prove anything like that if you design the thing right and… he was sort… it was true that people did sort of use the same kind of search, but it didn’t hold up in details, and what happened is they got more and more interested in being psychologists, and eventually the quality of… I think actually their research was better than ours at first, because they were better at getting these programs to actually work, and then they leveled off by trying to prematurely say: This is a good solid cognitive theory.

One day I went to visit her at her home in a house near the beach in... in Bell Harbor, New York, just a suburb of the City; and in fact I… we went with… I went with John McCarthy, and for some reason we had arranged for Gloria’s parents to think that he was the one she was dating, to keep them from focusing their attention on me. Then somewhat later, there was this… what I call the telephone story, which is that I went to visit Gloria at… when she was a resident at Bellevue Hospital in New York, and I happened to arrive when the telephone switchboard wasn’t working – a room full of nice ladies with microphones and switching calls around – and I don’t remember what was wrong, but I managed to fix the thing, so the calls started going through again. And after that, I’m told they didn’t send Gloria telephone calls from other young men but only from me, so that… that may have facilitated our future relationships.

Whenever something needed to be done or I thought it did I would mention it and then it would just happen. Whereas every now and then I run into some former student and they say: 'Oh, I remember that and John McCarthy had to do something to get that to happen', or Russell Noftsker or Tom Knight or someone else. So I had this illusion of everything happening very spontaneously and there were actually some serious administrative-oriented people actually filling out forms and persuading other people to get things done but it all seemed invisible to me.

One feature of… that goes through this, my later career, was that I never felt I was working very hard except on mathematics or something, and, for example, being director of this laboratory seemed almost effortless because I always had a co-director. So, actually John McCarthy and I started this laboratory mostly because... I was hanging around in... in the very small laboratory of Warren McCulloch who was a... a great pioneer in cybernetics and things like that. And McCarthy, who was a classmate more or less of mine at graduate school, had been at Dartmouth and I forget why, but for some reason he left Dartmouth and came to MIT and we started the Artificial Intelligence laboratory together.
In fact, the way it happened is we were talking about needing more students to work on different things and we ran into Jerry Wiesner who was... I think he had just become president of MIT but I’m not sure... right now and mentioned that we needed some support and he said, oh, talk to so-and-so. And that person gave us access to the computer and someone else gave us a room and all of these things happened. But the main thing is that sometimes I would do something or ask for something; sometimes McCarthy would and we were actually co-directors – there was no-one in charge.

Then a few years later, I think 1963 – four years later – McCarthy went to, moved to Stanford but I had met Seymour Papert; in a sort of remarkable way we were both at some meeting in England and presented the same paper on some kind of machine learning idea, but it was exactly the same fragment of mathematics that we had both discovered separately and it turned out that for most things, if you asked a question we’d both give the same kind of possible answer. So we became co-directors and then a few years later Papert started his own laboratory to develop educational systems for young children and one of my former students, Patrick Winston who was a very original creative theorist in artificial intelligence was there, and I asked him to be co-director.
So, through this whole period, if there was an important decision to make and I couldn’t make it, I would just go home and Papert or Winston or... whoever, the co-director, would make the decision and I’ve never understood since then why anybody would want to be the director of a company. And, in fact, generally, I find those people become rather... they enjoy the power and the making of unpleasant decisions and generally I don’t get along with them very well. But apparently there’s... there's two ways of running an organization and I’ve seen very few cases where there’s a pair of directors, but I think the Google twins might be an example and... so that sort of thing does happen here and there.

One of the reasons that if you ask me how did such and such happen in this laboratory, and why did things work out so well, I usually get credit for it. But in most cases I don’t know what happened because most likely the other guy… there was also a young fellow named Russell Noftsker who... we needed an extra engineer because there’s some things you can’t ask students to do... it’s too hard or they’d have to spend much time on it or they wouldn’t learn much from it. So to me it’s unethical to have a graduate student do hundreds of hours of manual labor. This sort of thing happens in biological laboratories and I think it should be banned and if you want somebody to spend many hundreds of hours being sure that a bottle of something is at the right temperature and is poured in such and such a way, you should hire somebody who’s not a student because what you’re doing is actually wasting…
But anyway, I was interviewing people for being a possible... professional engineer for some of our projects so the students wouldn’t have to do things that they couldn’t learn much from and we interviewed a few people, but this was the only one who seemed to… he had built his own auto gyro or some kind of airplane like that and actually flown it around in New Mexico and not been killed and... I thought that was excellent qualification for… and so that was another person who, in some sense, was in charge of lots of things that I wasn’t good at doing like financial administration and hiring technicians and so forth. So I get a lot of credit for having run this wonderful laboratory, but the trick was that I just came across various tricks for not running it.

Well, no, another topic is science fiction because one of the elements of a lot of things I did came from... as a child, reading science fiction. I don’t know if there was anything about that in Bernstein’s – Jeremy’s paper – but when I was a child I ran across HG Wells – it was a big book of HG Wells’ short stories and he was one of the pioneers of science fiction.  I certainly read the English translation of Jules Verne and the short stories of HG Wells which had all sorts of hyper modern ideas in them. There was one story in which some electrical accident had converted a person from being left... right-handed to left-handed and in fact, his whole world was inverted, and I think an element of this story was that he became ill because he was eating right-handed proteins. And I think that this must have been around 1900 at some point when it became known that, for some strange reason all the amino acids have a certain right-handedness and it’s just... it’s an accident that... for which there’s no modern theory why... why are all the proteins twisted this way, rather than that? And, well, it had to be one or the other and... might have been a meaningless accident.

Here’s HG Wells with his book of 30 or 40 little stories each of which has some amazing idea like instantaneous transformation of matter or... or whatever. But I ran out of those and just at the time when there was no more good… Aldous Huxley was another one – Brave New World – just when I’d read all the classics of which there are only about 10 or 20, then another... an Isaac Asimov appears or... and a Robert Heinlein – 1940 – and that renewed my interest and these people also had short stories and longer ones with new ideas about possible new sciences and new ways of looking at things and... the end result of that is that generally I’d read very little except science fiction and science. Every now and then I try to read a novel but to me all novels are the same, they’re about a bunch of not very interesting people who make some mistake and their life gets very complicated and at the end they either find a way to... to get better or they get killed or whatever. And it seems to me when you’ve read 10 or 20 novels you’ve read them all, but in the world… so they're... general literature is not general to me, it’s almost all the same whereas science fiction is the opposite and each writer tries to make something completely different.
So it doesn’t have Agatha Christies who write 50 wonderful stories that are almost the same with a little clue, but they try to have a big idea that… but then, by some kind of accident, also I happened to meet those people, so unlike with most science fiction readers... Isaac Asimov lived not so far away and I finally met him and I forget how I met Theodore Sturgeon and Robert Heinlein and Arthur Clarke, but they all became friends of ours and so that was my community. Mostly people interested in science and psychology and physics and so forth, but also the... David Brin and Vernor Vinge and Robert Heinlein and Isaac Asimov, the people who were writing the most imaginative… in fact Arthur Clarke himself lived upstairs in the third floor for a period when he was loafing in America for some reason.

Arthur Clarke had his own satellite for a period, was... was that happening when…? 
[Q] I don’t remember.
There were some Indian… there was some synchronous satellite over India which had ran out of fuel so that in order to keep a satellite at exactly the right place it needs to emit a little bit of gas every now and then and correct its orbit and eventually they run out in several years because they can’t carry an unlimited amount. So there was some Indian satellite had drifted so that now it was over Sri Lanka and nobody was using it; except the company that owned it... gave Arthur Clarke a communicator so that he could use it make telephone calls and things like that all over the world... free, but eventually it drifted further; it was going... gradually going east, got out of range. But I think for two or three years he might have been the only person on earth who had his own satellite, which is poetic justice since he was the first person to explain how useful they could be in the... must have been the early 1940s.

Yes, he had a… he was early and sort of pioneered... I liked the... the way Arthur Clarke treated the future better but Stapledon was maybe one of the first to imagine humans evolving into some more super intelligent form and undergoing inexplicable transformations.

Yes, Sagan was not a science fiction writer at first, although he became one. And we were friends when he was a young professor at Harvard and inexplicably he never got promoted, so I think Harvard didn’t appreciate the power of Carl Sagan’s imagination and scientific ability. So eventually he moved to Cornell and he invited me to give some lectures there which was a great experience to spend a week with him. And then as the space program expanded I went on quite a few trips with Carl to visit various NASA bases and see what was happening and... I was sort of informally involved in a fair number of Pioneer and Voyager missions, but just as a... as a sort of guest advisor.
The problem with dealing with NASA was that if you had an idea… for example, on the Mars Viking Lander which is a wonderful machine that worked quite well, it couldn’t move once it had landed, but it had an arm that could dig and it had a little telescope and at the last minute I realized that if you mounted a little convex lens on its wrist then if it really wanted to zoom in on something, since it had a camera on the main body, it could move its arm like that and look through this little magnifying glass and get maybe another five or 10 power just the way Galileo could see the moons of Jupiter with... with a single lens.
And the folks at NASA were very excited because this wouldn’t cost much. But the thing had to be launched in six months and according to their schedule of planning, you couldn’t make any change in that short a time because if you put any little thing anywhere on the thing you would have to completely re-compute all the dynamics and check whether this would interfere with any other system. And even though everybody said, they couldn’t see how it could do any harm, experience had shown that everything can do lots of harm.
But travelling around with Carl Sagan, just visiting things that were happening... it was like being in science fiction, not just reading it.

One of the things I was interested in was Arthur Clarke’s idea of this space elevator and so I must have spent about six months working with some scientists at Livermore who were thinking about designing such things. Because it’s possible, in principle, to build a kind of pulley – a belt – made of carbon fiber, some incredibly strong piece of wire and have this go up from earth to something higher than a synchronous satellite and down again. So you could have a pulley that would haul things into space, and Arthur Clarke had worked out the theory of that. I forget… he called it a fountain.
And then I worked out a form of it that might actually be practical. The trouble with the fountain is that if a meteorite hits the wire then the entire structure will be destroyed in minutes. And so... I figured out a system that involved a stream of separate gadgets; each were hooked by a flexible cable to the next one, but the whole system, in principle, could withstand an accident. And the scientists at Livermore worked on that – Lowell Wood was the... my principal collaborator – he was quite an imaginative, and still is... scientist engineer. And nothing came of it in the long run because... if you built such a thing it would be thousands of tons of equipment, well... going rapidly up into space and down. And if anything did go wrong, which it surely would eventually, the…
Arthur Clarke had imagined all this and... and in fact, he figured out that the thing would have to be somewhere on, a little bit east of India so that if the thing broke it would fall into the Pacific Ocean and not destroy any cities but... I don’t think any… it’s like the Orion Project which is a space vehicle powered by nuclear explosions and again, to build the space fountain would be wonderful because you could hoist things into space for just a few dollars a pound or less instead of tens of thousands of dollars a pound. But... but the risks in building it are probably too high for that to ever be made.
Anyway, the world of science fiction and the world of real engineering does sort of come together in the exploration of the solar system and things like that.

I think it was the thread of... saying, how can we... how can we make science fiction more real? How can we make space travel so economical that… right now only a billionaire can go on a ride or... I think... I think you can go into space for about $20 million, I forget.  Do you know, what the current price is? But the Russians will take you up to their... their station, our station. I’m not sure whose station it is. Right now the Americans are retiring the shuttle and we have no way to visit our space station except to pay the Russians presumably some huge price to go up on a... one of their rockets.

You know, I think that one could attribute this particular... the space thing to email because all of the calculations and discussions of this thing went back and forth, because if I passed a terminal and I had some idea I could send an email to Lowell Wood… I forget the other couple of guys who were involved. So you know, if you just got an idea you could type it over to those people and… so it might be that the medium was more of the message than the... than anything else. I’ve never been to Livermore, but it’s just around the corner when you have this keyboard.
Maybe that’s the answer; you have a friend and you know they’re interested in something and in the old days you would have to write a letter and put a stamp on it or something and it’s just too much trouble so you’d do something else. But... but now if you want to communicate with someone you just take out your iPhone and do it. So I’ve had some friends that I’ve never met.

I came to MIT in about 1958 or '59 and started teaching in the math department and accumulating these students working on ideas about artificial intelligence. And at that time, the capabilities of... what we call AI Systems – Artificial Intelligence Systems – were pretty limited. And we were working on… most people were working on pedestrian problems like: can you build a program that will scan a picture... most computers didn’t even have a scanner at the time so we, in fact, had to build those optical scanners or use facsimile machines or something like that. And could you build a machine that would recognize a handwritten word like 'and', and 'friend' and 'computer'.
So a number of people were working on just transcribing hand-printed text that wasn’t so hard; hand-written text was harder. Someone discovered that it was easy to read handwritten text if you could record the motion of the pen rather than just scan the picture. There were a couple of MIT professors who noticed that if you looked at the velocity changes of the pen, it was very... comparatively easy to distinguish an M from an N and an O from a U just by the... looking at the speed at which the pen was moving, maybe not even caring which direction it was moving, but just looking at the changes in speed – each letter would have a different profile. So, there were all sorts of strange little discoveries like that.
But for many years, in fact to this day, we don’t have systems that can read handwriting very well. We also worked on trying to recognize words that were spoken, where you’re taking an audio wave form and... that sort of thing. But in our lab... most of these things were happening in a few places around the country, around the world, really, notably England.

But we got more interested in answering questions that were typed into the computer. So I had three or four students who were writing programs to understand, to some extent, ordinary English printed text or typed... typed text; so there’s no pattern recognition problem, you just... the computer knows exactly what letter... what key has been pressed and so it’s not a matter of recognizing a visual form.
So there was one student named Daniel Bobrow who wrote a program that was fairly successful in solving first year high school algebra problems. So you could type a problem like this: the… a certain automobile can go... can get 15 miles per gallon; the distance from Boston to New York is 230 miles or whatever. How many gallons will it take to go... drive from New York to Boston or whatever. And Bobrow’s program was able to answer enough to score it a 9th or a 10th grade level for answering such questions. And it did it remarkably well considering that it didn’t know what most of the words meant. It knew that plus meant to add and so it knew a lot of ordinary words that had mathematical meanings and the program was designed that every sentence would be an equation – X=Y+Z – and it didn’t know what a car was or what gasoline was or even miles, but it knew what per means. Miles per gallon is a fraction. And this was a dramatically successful program because it, in fact, was comparable to typical not very good students in high school for that kind of problem.

And in the 1960s, from beginning in 1958, there were several such programs that showed… the most dramatic one was one that was good at formal... the part of formal mathematics called integral calculus and a young student named James Slagle wrote a program that was better than most students at doing what was called integration which was a process invented by Isaac Newton and had been developed for several hundred years, but it was... never been automated before this time. And a graduate student named Joel Moses, who eventually became Provost of MIT, and... very important post in the faculty, but that was his project. Well, James Slagle was the... the first one to do it and interesting thing about Slagle was that he was blind but by using a programming language invented by John McCarthy called Lisp – it’s a sort of joke, well, it’s short for list processor – and this was a very high level computing language which could do rather complicated things with fairly brief expressions.
Joel Moses, who was the student who followed Slagle, said it took him a couple of months to understand the ten pages of text that the blind Slagle had written to... to do this, because this language was so compact and expressive. So that’s a nice story in itself and maybe we should get Joel Moses to tell it. But anyway, this meant that in the early 1960s, for several years… I mean, 1970s, we had made a lot of progress from calculus which is... which was at that time generally a subject in college to algebra – mathematics – which is a kind of high school subject and getting down to elementary geometry, proving theorems which was a pre-high school subject or some cases. And so this was an interesting period of five or... five or 10 years in which evolution seemed to be going backwards.
Most people didn’t understand why would it be easier to write a calculus program than a geometry program? The answer is: you had to know more about the world; you had to know more different things for the subjects that people considered easier, but they were easier because they’re part of real life and people experience geometry but they don’t experience acceleration in... in the sensible way.

So there was a... interesting decade in which artificial intelligence started with very advanced things that most people considered hard and difficult, but weren’t. And we’re still not at the stage – this is 60 years later, 50 years later, anyway – there’s no machine that in any deep sense understands why you can pull something with a string but not push it. And of course the answer is that, once the string is straight it can't... its length can’t change so the object has to follow it or the string breaks. But it you go this way the length can change because the thing can curl up and it doesn’t take any force to push a loop and every child – every normal child – understands that sort of thing very well. But to this day no computer that I know of, no program understands that much about elementary manoeuvring in the real world.
One problem in the progress of artificial intelligence is that most researchers got stuck with this sort of problem; they tried to build robots that could do everyday things. So here you have this strange situation where, in 1961... robots, or rather computers, can solve mathematical problems that only very advanced students can do. Now if you look around the world you see robots trying to solve problems that every baby can... that four-year-olds, three-year-olds can speak a lot of language; they can build houses with blocks… we don’t have any robots that are... have the kind of general physical competence that a two or three-year-old has and I think the joke is that if those researchers would simulate the robots with stick figures instead of building actual physical ones with motors, progress would be much faster. Because if you have a simulated robot then one computer can support 20 students trying different experiments with it because a typical computer today could have as many as 50 or 100 different terminals with… because the computers are all time shared today and they can serve dozens or hundreds of customers.
If you build a physical robot in a laboratory... if you visit a robotics laboratory you’ll find that there’s usually one or two of the latest machine, it’s only working three hours out of every 24; it’s usually broken or someone’s improving it. So only one student at a time is doing some experiment that takes six weeks instead of five minutes, but people feel it’s not real if it’s just a picture of the robot, I think they’ve… it’s a community that has sort of strayed by mistaking certain... one kind of reality for another. It’s a... it's an interesting thing, that they don’t see the cost of confining yourself to a non-time-shared machine.

There was quite a lot of effort to build a machine that would recognize a face and in the last few years they’ve become fairly successful at it, I... I believe. But it’s not clear that they’ve learned very much about solving other problems. It would be nice to know how human brains do it and I don’t think there’s enough information yet, but I’ve seen descriptions of research that suggest that we might have special brain centers that have evolved for recognizing features of eyes and a few things like that. But for identifying people, fingerprints or eye... retinal eye patterns are pretty easily automated and pretty reliable, although... we often see convictions reversed because somebody made a mistake on the fingerprint identification.

One of the features of the lab, that when it... when we started up, I inherited a group of self powered students from the MIT Model Railroad Club. And it was interesting how computers developed at MIT because... the first applications of... the first non-numerical applications of computers really came from communities that you might not have expected, like civil engineering. People were developing new kinds of computer programs for designing buildings and in this civil engineering department people were, of course, using computers to do... solve some equations in physics and so forth. But... at least at MIT there weren’t any people who... or hardly any people whose main interest was in developing computers themselves.
So, in fact, the faculty had complicated discussions about, should there be a computer science department or should these things just develop as they were needed in civil engineering and electrical engineering and physics and chemistry and biology and so forth. And that’s what happened for many years rather successfully, that is, instead of having... hiring professors to teach about computers directly, it was decentralized and eventually it crystallized in several of the faculty getting together and starting a laboratory for computer science just at more or less the same time that we started the laboratory for artificial intelligence. And there was a little rivalry between them because the... the artificial intelligence people were trying to solve much less well defined problems than the ones who were trying to apply computers to particular very important and definite applications.
So there wasn’t too much communications between these two communities. But eventually they merged to a large extent, maybe for better and maybe for worse. I think for worse but that’s another story.

It’s that the artificial intelligence researchers were more... more flexible and experimental and if they weren’t successful on a particular problem they would move to another one, whereas the more professional people had to keep focusing on particular problems in electrical engineering or physics. And so the communities were different because the AI people were more like science fiction readers and writers and these people were more like business people who had to show a profit or had to get certain problems solved because they had to teach other students who would go out and become practitioners, so that you could say that, if you look at the world of research, there’s the kind of research where you’re exploring ideas that might take 20 or 30 years to mature and you can’t expect a company to pay for that because it will just be a minus on its balance sheet for as long as the stockholders… investors are looking at it.
Whereas, if you have an established profession then you can know how… what is the value of a particular advance and decide where to put your money and optimize things. So I think there’s a serious problem in the way governments and institutions have to allocate the long term basic res… when I say basic research we mean almost the opposite of basic. It means doing things where you’re not sure that they’re important yet. When they call it basic that’s an attempt to dignify it.

I know that in the 1950s and 1960s, for example, the National Institute of Health had 10 year fellowships; it was mostly five year fellowships, but it had quite a lot of money where, if you had a young person who had made some important accomplishment you could say: Well, we’ll give you a grant for 10 years. That was sort of the maximum I ever saw, but there were lots of five year grants. That’s gone now, I think on the whole, and people have to write a new proposal every year to get their money renewed.
So when I was a young student or a young professor there were places like Bell Labs. I think I mentioned this the other day but, when John McCarthy and I spent one summer at… of 1952 at Bell Labs they told us that… you should work on something that might take 40 years, not work on anything that you could do sooner because theirs was a laboratory designed for looking at those kinds of issues. The telephones themselves were designed to last 40 years – those black shiny things with the dial – and we still have one in the next room, I think. But there’s very little of that today; maybe a few institutions like the Rockefeller Foundation and… private institutions can… can do more of that. Polaroid - Land - tried to set up one with the Polaroid money, but… and it still exists. I’m not sure that it… it had enough funding to look at that kind of time horizon. But… but anyway, there were a lot of them in those early days. And, also the universities had that idea and they were expanding still in the 1960s, now they’re pretty compressed and professors are living longer and there are fewer jobs in universities for… for young researchers so things are a little tighter now.

Lowell Wood had a wonderful idea for launching… things that were fairly practical. Suppose you wanted to make something like the space station.  His idea was very simple. You… you drill a hole that’s… I think it doesn’t have to be very deep – maybe a mile – that’s a pretty big hole, and you put a hydrogen bomb… a little hydrogen bomb in it. And then you have… you fill it with water, about… I forget how much they… the Livermore people worked a lot of this out, and then you have this cartridge which is the… the space station. And the space station is made to fold up and just imagine that it folds up so it’s solid steel. Well, it turns out that if you make a solid packed station with… without any holes in it, it can stand 10,000 g's easily; that is, you can push it very hard and it won’t break, it’ll just rebound a little.
So, here we have a hydrogen bomb, a column of water which turns into steam and it propels this… this payload, so it’s a gun. Now Lowell claims that… that this hydrogen bomb will be completely sealed after it go… goes off. It’ll make... it’ll convert the surrounding earth into a spherical melted glass cavern and no… no harm will ever come from it. It won’t leak because it’s all melted and sealed and, of course, that might be wrong; and the whole thing only costs a few million dollars really because it’s like an oil well only it’s a big hole instead of… which might be easier than a small one in some ways.
And then you need some way to get the thing into orbit and so it won’t just come down again, so there has to be some kind of secondary system which is much more expensive which pushes it sideways so it goes around. Maybe it’s another couple of hydrogen bombs and that’s all you need and since you’re outside the atmosphere they won’t do any harm and hydrogen bombs only cost a few million dollars each. So here was a system that builds a huge space station for… for pennies or rather pennies per pound, so they worked all that and everybody said no. I forget why I’m telling this story.
[Q] Well, it’s very interesting.  How does the solid space station… does it unpack when it gets up there? Is that right?
Yes, you… just like the… the Hubble Telescope… of course, you… people come out and unpack it for the Hubble, but it’d be the same here; it would just… it just has a lot of hinges and parts and once you’ve got it into orbit then… then you send up a shuttle with people to… people can unpack it so…  and they, you know, it has enough food for several years and everything’s packed in there and it sounds pretty practical.

One of the big steps in my career was that I was a student at Harvard. I took all sorts of courses, mostly science. I managed to… you know, you take four courses a term and you’re, you have eight terms, two terms per year. So that’s only 32… courses and I think I managed to get through Harvard with 27 or 28 science courses and then... what do you do about the rest? Two were philosophy which was a course given by Willard Quine who’s a logician, so that’s… that’s actually mathematics – philosophy hides that way – probably three of them were music which is certainly not considered science, but there I was. And there was a young professor, composer, Irving Fine, who taught music and I think I took three of his courses and he gave me a C in each one. But he thought I was maybe the most talented of the students anyway.

So then in my senior year… I was… I was majoring in physics mostly and with fairly poor grades because I was doing so many other things… and I thought I would make up for this by doing a good thesis, but it turned out at Harvard, physics students can’t… they don’t have an undergraduate thesis, but they do in math. So, I asked Andy Gleason why can’t I just switch to math and do a thesis and he said: 'No problem,' signed something and I decided to do a thesis on fixed point theorems which was a beautiful little fragment of mathematics that… that I was very interested in. And there was a theorem that… Kakutani was a professor at Yale and here’s an example of a fixed point theorem: Of… consider that at each point in the world on… on the earth, there’s a temperature and there’s a humidity, and these vary continuously over… over continuous functions. So there’s two continuous functions on this sphere. Kakutani… somebody else had proved that… I forget who… Whitney, Professor Whitney, was a friend of mine at Harvard actually, had proved that you can find two points on the earth which are exactly opposite points which have the same temperature and the same humidity. This is rather surprising.
So that… that was a famous fixed point theorem. Or you could say there's, if you consider mountains…  There’s two points which have the same altitude and the same temperature, any two functions. It’s very strange, why should that be? Anyway, Kakutani had proved that if you take a… you can find a triangle, an equilateral triangle, that is… 60º angles, the same thing is true, for an equilateral triangle you can find three points which will have the same temperature and pressure… or temperature and humidity. So that was Kakutani’s fixed point theorem. He was a professor at Yale – I’d never met him actually – so that’s very interesting because it shows that somehow if you have an extra point you can… you can do a little more because now they’re equal at three points.  Isn’t that funny?
And so then I said: 'Well, what about other... why does it have to be an equilateral triangle?' And so I worked on that a lot and I managed to prove that if it were a right triangle, it was also true and also if it were a triangle which were three points of a regular pentagon, that would be… the same thing would be true. And that’s as far as I got, I couldn’t prove it for any old triangle. But, anyway, my proof of the thing for the pentagon impressed Gleason and that was probably why he told Princeton they had to admit me as a graduate student. That’s the next step for a mathematician.
So it was a great adventure and… but it always haunted me that I couldn’t show this thing for any old triangle.

[Q] Right, so you... you were haunted by the fact that you couldn’t make this work for any old triangle.
Right, but it seemed, if it was true for a... equilateral one and a right angle one and a pentagon... triangle in a pentagon, then it must be true for all of them, but I couldn’t prove it and then... somehow I got a message... I don’t remember how I learned but... that this fellow Dyson had extended this and managed to prove either that or a similar thing about four points on a big circle, a square. Anyway, it was clear to me that, if it was true for these three kinds of triangles, it must... there’s an extra degree of freedom here, that it’s true for a whole class of things and so maybe it’s also true for a particular set of four points instead of a lot of different sets of three points and Dyson proved that in a paper that I actually retrieved yesterday and...
But there was another mathematician who proved something similar around the same time.  But anyway, when I read Dyson’s paper I was so impressed that anybody could actually produce such an amazingly complicated proof of something at all and my reaction was not to learn how to do that, but to say: 'Oh he has a different skill set from me and I don’t have that particular skill set so I’ll stop trying to push this anymore because there’s somebody in the world who’s better at it.' So I’ll... I'll get... I’ll print out that part of Dyson’s... Dyson actually mentioned me and Kakutani in his paper of... as having started this thing.
And... so... anyway, it was one of my early experiences of saying: 'Oh look, I would like to do this kind of fixed point topology, but since there’s someone better at it, why...' It’s the cowardice principle: Why bother to do something if somebody else is already better at it? And so I’ve always been excited to see what Dyson does next and I’ve followed his career quite a lot so I did read some of the early Orion things, but I never actually saw the final result of the theory and the politics that must have gone into getting it dismissed. Of course once you’re on the moon you can do it, because there’s nobody to kill, but it’s pretty hard to get a lot of people up on the moon.

I’m trying to think of a few parallels, but generally, if you’re very good at something and you find somebody who’s better, then you should figure out something else your skill set is good for and it turned out that I was also interested in the foundations of logic and computation and I had discovered some interesting things about that... and then... a mathematician named Martin Davis, who had been a friend of mine for some years, told me about a problem that was... had been generated by a professor at NYU, Emil Post, in the 1920s and he wrote a paper in 1923 conjecturing something about a certain kind of logical system and... long before Alan Turing had published anything, Post had published something about how he thought maybe this was an unsolvable problem. Also Gödel had... Gödel was 1931 and Gödel had the first proof of what’s called a recursively unsolvable problem, but Post had sort of pioneered in that area in the 1920s and discovered something which he conjectured was unsolvable, but Gödel was the first one to prove anything and then Turing in 1936 – Alan Turing – had come up with a much simpler proof of an unsolvable problem that... something that a computer couldn’t do and I didn’t know about Post’s work, but Martin Davis had been a student of him at... at NYU and out of the blue Davis phoned me and said: 'There’s a problem that Emil Post and I couldn’t solve, but I think you might have the skill set for that.' The funny part is that I mentioned this to Davis a few years ago and he said: 'I... I never did that, I don’t remember telling you.' But I remember fairly clearly that I never heard of Post’s problem and that Davis told me about it because he thought that I might be able to solve it and indeed I did. So I published a paper in 1961 solving this interesting old problem of Emil Post.

I forget why I’m mentioning this, but...
[Q] Because it’s part of the cowardice principle. 
Yes, right. So here was a case where somebody else diagnosed that I might have the skill set for doing that and Davis had been trying to prove this thing for years and I believe I solved it in about three or four weeks. But, it was quite interesting and then having solved that I... I got some techniques which led to a couple of new discoveries and one was... one is called Minsky Machines occasionally and what I discovered is that you could make... there’s such a thing as a universal computer and Turing... Alan Turing was... and Gödel and Church – another professor at Princeton – had made some discoveries about these universal machines and so one question was... in a universal machine is a computer that can simulate any other computer if you write a big enough program for that. So the question is: What’s the smallest universal computer? And what I discovered after solving Post’s problem was that you could make a universal computer that just had one... one register or tape and all this machine could do is add one, it had... it could store two numbers, so it has two registers, they’re unlimited size, and the operations that... the instructions the machine can do is just add one to one register, add one to the other register or subtract one from either register and when it subtracts it can test to see whether the register is zero or empty and that’s all. So basically it’s... you can make a universal computer with just these two registers and four instructions.
So knowing that, lots of people have been able to show that there exist many other kinds of universal computers, but this was sort of the simplest one that started a sort of new field and the guy who’s exploited that field a lot is Steve Wolfram, he wrote a whole book about what you can do with universal computers of various sorts and... anyway, that came out of this just chance interaction where Martin Davis calls me up and says: 'I think you could solve Post’s problem', and I said: 'What’s Post’s problem?' And... and his guess was right.
So I think I’ve done the same thing sometimes for some students, but I can’t remember it and every now and then... like just yesterday at that dinner, some ex-student came over and said oh... I asked him what he was doing and he said: 'I’m... I'm the leader in this new field here and it’s something that you got me to be interested in.' So... so that’s a weird kind of social network where math... mathematicians find the... there are a fair number of cases where you find two mathematicians solving a problem that one couldn’t.

My friend Gleason was... there was something called Hilbert’s... Hilbert's Fifth Problem, which was proving that every continuous group is differentiable... it's something that was recognized as a possibility and it was solved by Gleason and Julia Robinson and I can’t remember the third person. But when I first met Andrew Gleason, when I was a sophomore at Harvard or a freshman maybe, I asked him what he was working on and he told me about this thing called Hilbert’s Fifth Problem and he sort of told... told me enough to... so that I could see what it was and I said: 'Well, do you have any idea how long it would take to solve this?' And he said: 'Yes, I think it'll take about eight years.' I wonder if that’s in Jeremy... Jeremy’s article. I’d never met anyone who ever said anything like that and I said: 'Well, what do you mean, how... how do you... why do you think that?' And he said: 'Well, I think I’ve divided it into three parts, and the first... first part is to show that in one of these continuous groups you can make something called Dedekind cuts.' Dedekind was a mathematician who first developed the theory of continuous functions and real numbers and stuff like that. And he said: 'And that’s a pretty complicated construction and I think it’ll take about three years', then... once you’ve got that, then you... you construct lines – continuous lines – and then you can show that these curves are continuous and differentiable or something, I forget the other steps. But the second step is two years and the third step is three years and in fact it took eight years, but he had help on the third step, that was where Julia Robinson came in and... but it was about eight... seven or nine years anyway.
And... I... I don’t think I’ve ever heard such a prediction ever again since. Maybe somebody working on the genome would say: 'Well it will take about eight years to do this because it’ll take three years to make this machine that holds the chromosome and about two years that’ll make something that’ll grab the end and pull it through.'  So I can imagine for physics problems that you could estimate how much equipment you’ll need, but for mathematics it seems really... wonderful.
There’s a thing called the Putnam Competition every year in... in the US. I think you have to be a US citizen to enter it and young mathematicians enter this contest every spring and Gleason took first prize three years in a row and so he... he already had... was established as being one of the great all time problem solvers.

Right. OK, well I told this little story of the attempts to make programs that do mathematics, so that’s sort of the strange backwards evolution where, in 1961, the student James Slagle made a program that could do integration, as well as a good college student, and then Daniel Bobrow, another graduate student, made the program that was pretty good at some high school algebra problems and... while all this was going on, there was also a series of attempts to make programs that could prove theorems in Euclidian geometry, so that... that had a nice history going back to around 1958.
So we have these rather advanced subjects, at least as a typical public citizen would see it... doing calculus or algebra is pretty exceptional, but we don’t have a system that understands enough about the real world to... to know that you can pull with a string – my favorite example – but... but you can’t push or... knows that normally people don’t like to get wet, so they... if they’re going out in the rain they’ll probably get an umbrella, but they won’t take an umbrella when they go swimming, because then.. then they want to get wet or they don’t mind it.

To this day, you can find computers that can do many specialized things like making airplane reservations or calculating the rate at which two chemicals will react or solve various kinds of physics problems and other scientific things or economic ones, but there are no programs that could read a simple story, like an Aesop’s fable and say something about what’s the moral of this tale or what kind of person would be pleased to hear that story and who would be annoyed or just generally, there’s no computer program that knows anything much about people or the practical objects in... in the real world.

One question is why, if we were making such rapid progress in the early 1960s, solving college level problems in... in scientific fields, why can’t we do... 60 years later, 50 years later anyway, why don’t we have programs that could answer the kinds of simple questions that any two or three year old could do and I've a number of ideas about that. In fact I’ve written two books about what I think one would need to achieve a certain level of common sense thinking ability and I have a bunch of theories about why we don’t have such systems and the most interesting question is... is simply, if you observe that – so far as I know – there are only about 20 or 30 people who are being supported to spend full time making theories of normal, smart human reasoning.
There are tens of thousands of people who are working on particular ideas about that, rather than general ideas. For example, how do you make a system that evolves by trial and error to simulate certain aspects of evolution or how do you make a machine that tries to calculate the probability of certain kinds of events, given a large amount of data about related events. But there are almost no groups or individuals working on how do you make a program that makes really productive analogies so that if it sees one thing working it can say, I know how to apply something like that to a somewhat different problem. It seems to me that we have tens of thousands of highly educated people working on trying to apply logic to thinking and it’s just the wrong thing to do. I don’t think people use anything very much like logic. What they do is a lot of analogy. This situation resembles this one in some ways but not others, which of these ways is important for solving the problem I have, let’s reformulate and find a new description of this that emphasizes the features that might be relevant to the new problem I’m trying to solve.
This is the sort of thing you... you probably do something rather complicated like that every two or five seconds. When you walk through a room, you recognize certain kinds of things as... as harmless, other things as obstacles that you’ll have to get around, yet other things as being particularly fragile or expensive, you don’t want to damage them and... a great many different kinds of awarenesses, only a few of which actually come to the level where your articulately expressively thinking about them in relation to some other problem that you’re trying to solve. And as far as I know, there are only about 10 or 20 or maybe 30 people in the world who have the leisure or opportunity to try to make theories about common sense ordinary thinking.
We have tens of thousands of paid... highly paid to try to make predictions about the stock market or about the not stock market. How many groceries should I order for my store? Is there a business system I can buy from IBM or Microsoft or somewhere that will allow me to increase my profit by the 1%, which is the difference between gradually losing things and gradually gaining them? Tremendous energy on just barely significant applications that allow us to get by rather than fail, very little work on saying, how can I... how can I understand intelligence so that I could save children four of the 12 years of elementary school? Why... why can’t they go to college after eighth grade? We’re spending a large fraction of everybody’s life on things that haven’t changed for 100 years and nobody’s being paid to work on those sorts of questions.

By the early 1960s I had a collection of ideas about how intelligence works and I tried to write a book, but I didn’t know enough, so I wrote a rather long paper which was called Steps Towards Artificial Intelligence, but it really was steps towards theories of how thinking works. And it was quite a long essay with sort of six big chapters and each of them suggested a... a couple of areas in which we should do research. Like how do we... how could we understand... how could we make machines that could recognize different kinds of patterns and... and find what’s significant among a... a chain of events or how could we make a machine that could understand the meanings of words or... and things like that. And for the next 10 years or so, we had quite a number of students who evolved different theories about and experiments and computer programs that could do one or another aspect of... of these many different aspects of... of common sense and ordinary thinking.
Now... this continued to develop in many different ways by wonderfully talented young people till... till the middle 1980s and we were having some influence on a... research in a few other universities and laboratories around the world.

But we ran into some strange unexpected problems. For example, in the... in that wonderful period starting in the 1960s we had a couple of programs that were really, seemed... that... we had a couple of programs that seemed quite promising as steps towards understanding language and making machines that could communicate with people about ordinary things. Of course it was easy to get computers to communicate about highly technical things, because in a sense, things that are highly technical and advanced are usually much simpler than the things that you encounter as an ordinary person in the ordinary world.

For example, to understand modern physics, there are about 20 little well defined problems in mathematics or areas of mathematics that you have to understand: acceleration and velocity, the kinds of mechanics that began with Galileo and Newton can be compressed into a... a small book called Classical Mechanics and you can take a course in college and learn about gyroscopes and all sorts of different kinds of machinery. In some sense, that stuff is very compact and it ends up with just a few principles. The same for electricity, in the late 19th century, the great scientist... Clerk Maxwell found four laws, just as Newton found three laws for mechanics that explained most ordinary phenomena in... to some extent, Maxwell found four equations or principles that explained electricity and magnetism and that was a wonderful thing.
So now most of everyday physics could be partly explained at least by these seven laws, the three Newton and four Maxwell. Around early 1900s, Einstein squashed the Maxwell equations to be one or two rather than four, that was an amazing little bit of progress because he discovered that electricity and magnetism are... can be seen as the same thing from different points of view, completely unexpected kind of development. So physics got simpler and simpler and science was wonderful and beautiful and chemistry was beginning to be understood too. Things went... got into trouble in the 1920s when instruments became... new kinds of instruments were developed that showed that there were a lot of much less predictable events when you started to look at very small things and the part of science called quantum mechanics started to appear and raised some really difficult questions that we’re still making progress on today. So, we've been... that sort of set things back for almost 100 years by making the world seem much more complicated than it had after Newton and Einstein.

And when I’m talking about making machines that have common sense and solve ordinary problems, everything I say about those future computer programs are also things that you could ask about how do people do those things and what kind of machinery might be in a brain that... that allows us to do those sorts of things? And what’s bothering me is that when I wrote that paper called Steps Toward Artificial Intelligence, it was... it was something like 100 pages of many new... many old and many new ideas about how those sorts of things that might work and then around the world, mostly in America, but also in England and few other places, more of these kinds of theories started to appear and psychology sort of developed a new branch which is today called cognitive psychology, as opposed to behavioral psychology or I forget the names of all the others, psychiatry and so forth.

Some troubles appeared and the picture of this wonderfully developing science, it was called cybernetics in the late 1940s, the idea of machines that had... that behaved as though they had goals and purposes to some small extent and all sorts of mathematical theories of that and how that might be used for machines. In the 1970s we had made progress on getting machines to understand the meanings of some words and sentences and a certain amount of linguistics. But strangely, by the 1980s, a lot of institutions that thought that they were working on new advanced theories of psychology, seemed to be going backwards and looking for theories of thinking that were more like Newton’s laws, going back, can I find three principles to guide learning, rather than three or four hundred.
If you look at the... what we know about the function of the human brain, and my favorite example is to take a big book on neurology and look at the index, you’ll find the names of maybe 200 or more different parts of the brain that have been identified, at least for the present, as being involved with slightly different or grossly different functions, so... some parts are clearly involved with language and others are clearly involved with vision and hearing and different sensory systems, some parts are involved with planning ahead for motor activities and so forth. And we know that you don’t want to describe what the brain does by anything like three laws or seven laws, because that wouldn’t account for why there are more than 100 different... different kinds of computations going on in the brain.
I have a lot of friends in neuroscience who say: 'Oh, don’t worry about that, there’s a lot of evidence that all of the different areas of the brain are basically very similar, they have almost the same structure, they’re just in different places and just connected to different things.' And, yes, you could say that. You could say all people are almost the same, because they’re all between four and eight feet high and they all weigh between 50 and 500 lbs and there’s very few differences. So it makes me nervous to see neuroscience moving in the direction of saying: 'Look, all the different parts of the brain, they’re all made of the same kinds of cells – neurons – maybe axon... maybe neuro... glial cells and so forth. So you know, in fact there’s a very popular theory that says almost all parts of the brain are practically the same and I don’t want to go into that.
But it’s clearly to me, what you want is to elaborate theories of the differences. Why are some people so much better at some things than others and can you correlate that with small differences between parts of the brain, not... not saying: 'Oh, they... they must all be doing virtually the same thing.' Maybe they are, but it’s the differences that matter.

And a particular thing that bothers me is the lack of progress on understanding how language is generated and produced. 
I think there’s a wonderful accident of history that, in the 1960s... I guess the name Noam Chomsky is well known here, he... and working with some mathematicians, found some really very nice simple theories of how grammar works and they accumulated evidence that grammatical processes are relatively simple, they’re almost the same in all languages and that led to a sort of Newton’s law idea about linguistics, that languages have a grammar and their grammar guides the kinds of expressions that can be generated and also guides how you convert a sentence back into a meaning and so forth. As far as I can see, this is a very superficial theory, probably completely wrong. The appearance of... but it spread throughout the world and so if you look in 1950, every university had a linguistics department with people making theories of how words had meanings and how... how symbols served as signals and there were... basically there was area called semiotics and semantics that became professional interests of many people.
By 1980, almost all academic theories of language and meaning had disappeared and were replaced by this very beautiful and elegant and irrelevant, I think, set of theories by Chomsky and his friends and... which were trying to replace the theory of how language means things by this other theory of, what was the structure of sentences, why does every verb have a subject and most of them have an object and how do adjectives modify a noun phrase and so forth. That’s very nice, but I think those theories are not relevant to the processes that are actually used to take... to take apart a sentence and connect the words with representations of their meanings and... and so forth.

What happened is that you’ll find people who are rediscovering the linguistic theories of the 1940s as though they were completely new and what I’m worried about is that... just as in 1960, I... 1970s, we were developing new theories of artificial intelligence, we’re now entering a... a strange age in which people are building rather dumb robots that have beautiful facial expressions but have no idea what they mean and are condemning students to spending vast amounts of time trying to get these relatively simple robots to appear to do more complicated and more meaningful things.
So I'd like to see some changes here and the main problem I think is... is a larger problem, at least in the United States, that we don’t have very many ways to spot a promising student and offer him or her an opportunity to spend five or 10 years pursuing the most promising long shots. So the... the most able students right now have very little choice but to work for a law firm or a bank or something that will make money within the next year, at the cost of this very smart and original student not developing a new, novel, original, hard to prove theory that might take another 10 years to start yielding results.
So the world depression is bad in... in not just the usual ways people think of, that people don’t have enough money to get enough food. It also means that smart people aren’t given enough time to think deep thoughts, because they... nobody will pay them for sitting there apparently doing nothing for four or five years.

In the '70s I had this description in Steps Toward Artificial Intelligence of four or five fields, of pattern recognition and what are we going to replace... what kind of reasoning should we use instead of logic and things like that. Then by the 1980s, working with Seymour Papert, we developed the view that... doesn’t seem very radical now, but the idea was: Psychology should not aim to be like physics, because in physics... the great progress in physics was a series of miracles where it turned out that things that looked very complicated could actually be almost completely explained by three laws of Newton or four laws of Maxwell and this was a revelation that... that a set of equations that would fit on one page could describe such an enormous fraction of all the things that... that were going on in the world. Of course they couldn’t describe particular things, but these were properties that all those things shared and the view that Papert and I started to develop was that psychology was different from that and that we had many different ways to think. So instead of trying to find, what are the basic principles of thinking... we would ask: What are the hundred most useful ways of thinking and how do they relate and what... what manages them, how do you decide which way to think when you’re in a situation? Should you think about the thing mathematically? Should you think about it as a set of... like a model of a human society of different things interacting with different goals and so forth? And Papert and I developed what we called the Society of Mind idea, which is that we’re not looking for a single theory of psychology, but we’re looking for something like a dozen or a hundred or nobody knows yet how many different ways of thinking are there and then once you have that idea clearly that sometimes you’re going to use logic, sometimes you’re going to use analogy, sometimes you’re going to use trial and error of some sort, all these different methods, then what manages them? Is there a next level which keeps track of which methods have been working on this kind of problem? What kind of problem is this? Is... should I change my way of describing the problem so that different methods get activated?
And we began to get this picture that what psychology needed is something more like an administrative theory than like the old theories. The old theories were very, very simple, they said: How does memory work? Well, the things that happened most recently are remembered most clearly. If there’s a long series of things, you remember the things at the beginning and the end more clearly and you lose the things in the middle. And people began to try to describe memory in terms of four or five laws. Well there was one psychologist named Hull who got good at that, but he discovered that, after a while, he had... he had something like 120 laws by the time he gave up and said: 'No, it’s not like physics, these other behavioral... these Newton like psychologists aren’t getting anywhere, I’m getting somewhere, but I can’t keep track now, it’s gotten too big.'

Well, what Papert and I did was say 'Well yes, it’s going to get pretty big and then we’re going to have to make administrative theories of, how do other levels of the mind manage all those resources so they don’t go crazy and one of them doesn’t dominate the others and how is all this stuff regulated?' Well I kept working on that and then at some point in the early 1970s, Papert began to think well, we’ve got to verify these theories and see if they work, and so instead of working together all the time now, we were being more separate. And he started groups of training people to work with children and see how... how their learning actually worked and trying to give them new kinds of tools... especially the first... this was the period when computers became cheap enough that, for the first time, children could be given access to them and Papert pioneered that area and discovered that fifth graders could... were incredibly productive and original and in a few hours many of them became very good at building complicated programs and making machines do really interesting things.

And so then our worlds started to divide a little bit. I wrote the book called Society of Mind and Seymour wrote several books of theories about how children develop and what might help them. And he tried to start a movement in which he would train teachers to develop this kind of constructive point of view where children would actually think about problems and build models of them and experiment and be like little scientists or like little researchers and the hope was that you train a few teachers to do this and they’ll train a few more and we all hoped that in a decade maybe there would be thousands of these groups and these new ideas would spread and spread and spread, but... didn’t really work that way, for reasons I can’t understand. If you train 10 teachers then the next year they might train nine others and the next year there might be seven or eight and instead of spreading like an epidemic, like the flu, these efforts sort of ran along steadily and then died out. I have no idea why they didn’t become more popular or what went wrong.
But, anyway, I kept working on the basic theory of how large numbers of different ways of thinking could be organized and Papert worked on more trying to see how do these actually work in children and what can be done to help them develop better strategies and tactics and so forth and I hope all this will come together again in the next... next few years, but I’m worried that with the economic depression and the limitation of resources that... that my fifty years of research was a golden age that might be tapering off and I’m just hoping it will start up again.

In the 1970s... while working with these students and with other... a few other researchers like... mostly Seymour Papert, who was my partner in a lot of these... these researches, but also some of our former students like Gerald Sussman and Patrick Winston, we developed all sorts of little theories about different aspects of thinking and theories of how... about how people recognize situations, how people make descriptions and represent memories and just so many different aspects of psychology that needed to be explained. And I made several attempts to… to describe these ideas, so I wrote a paper about a certain way of representing some kinds of knowledge in what I called frames, because they’re sort of like pictures enclosed in a... in a box that is... that you can file away and bring back. And then I had other ideas like scripts, which had also been developed by a friend named Roger Schank at Yale, and that was the idea of describing a process as a series of actions or a steps or situations and he worked out all sorts of ways that you could use these scripts for thinking or for solving problems, that’s another representation.

As time went by, we developed maybe a dozen different interesting ways to represent knowledge, as language expressions, as diagrams, as what are called connected graphs and as neural networks, there are maybe a dozen important ideas that have developed in various times in the history of psychology and more recently in computer science; different ways to represent knowledge, different kinds of operations to use knowledge to solve problems and so forth.
I tried to write articles about how to organize these new ideas into some sort of theory that people could… could use to try to explain how thinking works and try to make new theories that worked even better, but I couldn’t find any way to organize so many different things into a nice story that had a plot. And so I tried to do exactly the opposite at some point, which was to... if there are a hundred ideas or so, can I write a hundred little explanations and have each one point to a few of the other relevant ones? Well I started to do that and I might write 20 different one page essays on different ways to represent knowledge and 10 or 20 essays on different ways to use knowledge to solve problems and 20 different ways to decide if you have several problems, which ones should you work on now and so forth.
And eventually this ended up as 300 little essays and all of them were ranged from two or three paragraphs to two or three pages and in fact, at some point, I called on various friends to help say what order these should be in and can you see any way to make these into a chapter and people argued about it, but I finally decided that the best thing would be to just put them in some kind of order that made a little bit of sense, but not try to trace all the relations between them, so that became a book of about 350 pages and with a few exceptions, you usually can take any page and read it and get some idea of, here’s an aspect of psychology that you might wonder about and here’s an idea about one or two ways it might work and here are a couple of things it might connect to.

I wrote a second book which tried to reorganize this and some new higher level ideas, into a more systematic description, so that new book, which is called The Emotion Machine, for political reasons, it actually makes fun of emotions. But that book has more or less 10 big chapters, each of which tries to tell a particular story about certain ways of thinking and how they might work and what’s good and bad about them and… and how you decide when to use one.
So that’s more like a conventional book and my impression is that, in the end, it’s much harder to read, because… because the world doesn’t really have a plot and when you try to help people by giving them a plot, you might just envelop the whole thing with…with one really large idea that’s probably wrong and... so instead of learning the 10 important different things that got condensed into that chapter, you’re learning one really not very good theory about trying to explain how they all relate. So it’s a tough one and I think in general it might be that psychology is not the kind of thing that can be explained by good stories, much as people wish it could.

Yes, the Society of Mind was rather successful. I’ve had messages from people all over the world over the years saying this changed my life and… and I think the reason for its success is first, the chapters are short and each chapter has one main idea, or most of them do, and also the vocabulary I trimmed so that it doesn’t have many words that a ninth grader doesn’t know. So it's… it’s a pre-college book in the sense that its vocabulary, except for a dozen words – new words – is pretty limited. But the other thing is that the book itself illustrates the theory of the mind, that… that the way the mind works, it might have a couple of hundred different pieces of machinery, each of which occasionally interact with some of the others. And so the book itself is a... tries to be a… an analogy with how I think the brain works; mainly, here’s this thing that it does, it’s good at this kind of thing, and every now and then it calls on this one to take over or criticize itself and so forth.
And, so the new book, The Emotion Machine, has been out for three years and I’ve gotten a rather small number of messages saying: 'This was very exciting to read' and said… they’re very respectful, and they say: 'This looks like it has very important ideas and I’m already half way through it.' The letters about the… that’s The Emotion Machine, but the letters about the Society of Mind book says: 'I couldn’t put the thing down and I read it every year and it’s changed my life.' And I'm sure that the ideas in the new book are actually better, but they’re harder. And… I hope somebody comes along and rewrites it someday.
Incidentally, Michael Crichton helped me on the first book and he read the first 100 pages and had quite a lot of suggestions. He took out all the 'verys', I didn’t realize that if you say something is very important, that’s much weaker than saying something is important, but Michael took out a lot of qualifying words that... and he had several technical ideas. I’ve forgotten exactly which ones they were, but he actually had some important contributions that I rewrote into the book and forgot to credit him with.

I think something like the Society of Mind theory is necessary if you’re going to try to understand anything like human intelligence. Just as you need some theory like that to understand any aspect of biology... 'cause consider any animal. An animal is made of organs, a liver and pancreas and stomach and lungs and heart. And each of those systems has evolved to be good at some particular way of doing something or some particular function or solving some particular problem. Of course, they’re all related to survival, but that doesn’t mean that the animal has a survival instinct. It’s a wonderful paradox. Many people say, well the important thing about a living thing is that it has a survival instinct. That’s why it eats, to stay alive. And that’s why it reproduces, so that the species... but there really isn’t any survival instinct. There’s just a collection of mechanisms, all of which help to solve problems in different situations. It seems to me that it’s the same with the mind. That in order to survive or whatever, you have to solve and deal with different situations. And each situation involves some collection of problems to solve. So, a successful animal or the product of an evolution is almost... when it’s successful, is to develop a society of different methods for dealing with situations.

I think early 1940s a Japanese scientist named Susumu Ohno wrote a book called Evolution by Gene Duplication. And his central idea was that you don’t improve things by making small changes exactly. You don’t modify genes so often as you make new ones by copying old ones. Or in fact, I think major improvements or advances in biological species might be by having an entire extra chromosome, a copy of an old chromosome. Each human has 23 pairs of chromosomes. And, I think chimpanzees have 22 or there's... there was some accident in which one of the chromosomes broke up and duplicated and divided and went on a different course of evolution. That’s a bad example. But anyway, what’s the point? The point is that if you evolve by making changes in genes, then for each advance you’re going to also have a step backwards. Let’s take a nice example. Imagine that there’s a certain gene which is expressed both in making your human brain and your human heart. There are lots of genes that are active in... in many different organs. Well now, suppose you make a little change in this gene that makes the heart a little better at pumping blood. The chances are that the same gene will do something negative for its function in the brain. Because after all, at any moment in evolution almost every gene is almost optimal. The point of evolution is that you’re always making small changes in genes. I didn’t mean to say they don’t happen. And then the ones that tend to make the animal live longer or have more children – which is... that’s even more important – are kept. And so. we’re always evolving genes that are on the edge of being as good as they can be. Now, if you mutate a gene to make the... if there’s the same gene in the heart and the brain – and I’m sure there are hundreds of those – and you make one that makes the heart valve more efficient so it can pump more blood, it probably makes some parts of the brain a little less efficient. So there’s always a little bit of loss when you have a gain, almost always. And so, that’s why evolution is slow, because it has lots of genes, all of which are optimized. And we know that most mutations are bad. In fact, a very large proportion of changes in genes are fatal because they prevent the embryo from developing into an adult.

What Susumu Ohno pointed out is that... instead of modifying a gene, what if you duplicate it? And I... let’s carry that a little further and say, maybe on rare occasions, instead of duplicating a gene, suppose you duplicate a whole chromosome. There are many plants, for example, in which the entire set of chromosomes got accidentally duplicated. And, some plants’ seeds are just full of chromosomes because this happened many times. Anyway, you see, the point is that... now, if you have two copies of the gene – and preferably on different chromosomes, but not necessarily – now if you mutate one of them, there’s a chance that that could improve the heart without damaging the brain because there’s still the other copy of the gene making the function that works. The chances are it'll be slightly less good because the new gene will be not optimal, but at that point evolution can start off almost anew because now this gene can go this way and make the heart valve even better without hurting the brain. Presumably, this one drops out after a while and this other one can now make you slowly get better at learning or predicting or whatever function of the brain it’s doing, depends where it's expressed or... so that’s... Ohno’s idea. It’s a wonderful point. So, he’s saying that duplication is the most important thing. Optimization is what Darwin talks about – except of course Darwin didn’t know about genes – but he had a sound intuition about something like that. So that’s the big idea and that’s why I think we can assume that the Society of Mind theory is basically correct. How did people get so much smarter than chimpanzees? And how did chimpanzees get so much smarter than monkeys? And how did monkeys get so much smarter than – I forget what – squirrels? And, the big question is... I'm... I’m sure everybody knows that in the end we’re all descended from yeast. 'Cause yeast was the last thing between plants and animals. The... there was a duplication or a diversity operation and these went off and became plants and these became animals and we’re down here somewhere. It’s all part of a wonderful adventure. But the idea is that... if this is true – and it’s hard to see how it could be wrong and Ohno has a whole book showing examples – the best example is if you look at the spinal column where you have... I forget how many vertebrae there are – about 30 – but he traced the origin of the history of the genetics of the different branches of the spinal cord. And there are four or five groups. And in fact, it’s known that where the duplications and divisions happened and you can trace the differences between the nerves that come out of the different spinal segments in terms of the history of the way that the spinal column itself evolved. Anyway, it’s a beautiful story. And that’s another biological – rather than psychological – reason why something like the Society of Mind theory ought to be true, but you don’t see this approach in psychology. Somehow, this kind of thinking really doesn’t appear yet in what’s taught as psychological theories today. I’d like to see more... more thinking about that sort of thing.

I’m trying to remember... one day, he just turned up. And this was before he developed the fear of flying, I guess. 'Cause... 'cause... and he was going to make this movie and he wanted to know the state of computer graphics because he thought that this movie would have the latest in computer graphics. And he had heard that we were... had some and that Livermore had some and a few places... University of Utah had some. And so, he was making a little world tour to find out what could be done, because he... his plan was to... if there was really good computer graphics, then a lot of the movie could be made of that. And as you probably know, he found out everything that had been going on and... the end of the story is that he decided the animators... he made the correct decision. The animators were better and all of the little flickering things on that screen were drawn by hand by professional artists and animators and they came out fine and that was that, but he also was intrigued by artificial intelligence and of course, Arthur Clarke had been interested in that all along and... that was woven into the story. So, he invited me to come out to the... what was it called? Yea, something... to the studios. And... it was fascinating because first, he wanted me to play chess with him and I turned it down because I had a history of not playing chess very well. And, but we talked a lot and he didn’t tell me anything about the story exactly, but he wanted to get the background right so... the main example was, 'Will... by 2001, do you expect computers will be able to speak coherently?' And I said: 'I should think that by that time their voices will be very good and they’ll be nicely articulated, but I doubt that they’ll understand what they’re saying very well.' And that turned out to be true as a matter of fact.

OK, well anyway, one day many years later, he called up and we were talking about the... the AI movie, and... but I asked him what else he was doing and he said: 'Well I'm just... I’ve just been very worried about the world political situation,' and he said: 'Do you know that the Americans and Russians agreed to cut the number of atomic bombs from 40,000 to 20,000 recently?' And... and he said: 'This is really pathetic because it can’t really make any differences as long as they all have enough to destroy the world and...'  He said: 'I wish... can’t anybody get through to them to realize they’ve got to... do something much more effective than that?' And there was a long silence and then I said: 'Well, many years ago you made that movie called Dr Strangelove, which explained that just more clearly than anybody could possibly do, and if that doesn’t work, I don't know what would. There was another long silence and he said: 'Oh, I forgot about that.' I just... I just love this true story because it had these two long silences.

[Q] What was he like then to be with? I mean... well...
The amazing thing about him was that you know he makes movies. And, Super Balls had just come out. So I had a pocketful of Super Balls and we were walking along and I took one out and started bouncing it and Stanley was just completely entranced and he grabbed the Super Ball and he bounced it and it went way up and then, he threw it over a house and ran around and caught it back and... for the next 10 minutes, it was nothing but Super Balls, but... the part that amazed me was that... we... we kept walking around this place and talking and he kept bringing out this 35 mm camera – I think it was an Argus or something, a little cheap –and he’s always taking pictures and I’m thinking: This movie maker actually likes taking pictures. There’s something wrong with this picture.

So he wouldn’t tell me the plot, so I didn’t know much about HAL except that there would be an intelligent machine in this thing. But there was another little scene where... I described... I had an article... an interview so it's all written up somewhere. But... there was one point where we were actually in a... we went into a little theatre and there were some people that Stanley had gotten to... to look at this preview of... and the preview was a part of the light show. Remember the... that marvelous thing. And... oh and somewhere, and there was a picture in... and the... the artifact was a tetrahedron, before, it was a slab. So he was showing this crater on the moon and there was a tetrahedron, which was black. And, but it's a triangular base, it’s not like an Egyptian pyramid, which is a square base. A tetrahedron is this triangular base thing. And somebody... one of the consultants or whatever was saying: 'What is... what is that thing?' And Stanley said: 'That’s a tetrahedron.' And... and the visitor said: 'What shape is it?'  He’s looking at it. And Stanley realizes that ordinary people have never seen a tetrahedron and they don’t know what it is and they’re a little bit disturbed. And he scrapped it and made it into the... this is a... to me, this is a bizarre event because the idea of somebody not recognizing a tetrahedron is pretty alien, but Stanley regarded it as... oh, I don’t want anything in the movie that they don’t understand.

He did... he did tell me something about what was happening in the movie. And, I said: 'Oh, that could look like this.' And I drew... just on the back of – literally on the back of an envelope, I think – I drew this... these hands coming out of the module. And they’re exactly the ones he built. 
[Q] So, that was fun. 
Oh yeah, it was about three days. Then... didn’t see him again for a long time, but Arthur Clarke came over here and as I told you, he stayed with us for a few days.

That strange period around the beginning of the artificial intelligence laboratory when I spent a lot of time... sometimes I went on trips with this wonderful man and Warren McCulloch had – at that time – he was already... he was one of the great pioneers of cybernetics. So, in fact before Norbert Wiener did... wrote his famous book called Cybernetics, which I think came out in 1949, McCulloch had been working on neural networks – maybe the first – and he had a little group of... he had a 14 year old prodigy named Walter Pitts in Chicago and they worked out this paper that was published in 1943, so, that’s many years before Norbert Wiener came on the scene. And he had been a psychiatrist, a physician... neurologist really, and started the neuroscience laboratory at MIT and was a very dramatic world class actor figure. And I went on trips with him and... and it gave me a new view of the world because... when McCulloch... I remember one day he said: 'I’m going to explain something to the psychologists.' And he had a sort of grand view of... of this... the importance of cybernetics, which was correct. Otherwise, you would have said he was delusional.
And so, I went with him and it was a little meeting with about six or seven people. But he was on the stage and I realized that he was talking to the whole world. Normally, people are talking to their audience, but he would have very elaborate constructions and beautiful ways of saying things. But I had never met anyone for whom all the world was a stage; I think some... as someone put it. And from that, I got some sense that you shouldn’t waste people’s time with things that aren’t very important. Of course, nobody... nobody could live out such a commandment but, I must have spent most of a year just hanging around him and trying to understand how he could... see such importance in ordinary things. So in a way, it’s like Feynman, who would look at a little wave on the water and understand how the universe works. And McCulloch was very much like that for psychological things. He hated Freud because – in retrospect I think – because Freud was the best psychologist before him. And, you know Freud actually invented various forms of artificial intelligence and unlike the behaviorists, Freud thought of the mind as really a rather complicated thing not... not just a little feedback loop or something and... and imagined that it had many different structures which weren’t all compatible with one another and... so he... in retrospect, he had the first society of mind because it wasn’t just the conscious and unconscious, but he also had these sensors and whatever. Anyway, that was a... about a year when I got attached to this McCulloch and somewhere I wrote, maybe 100 years from now he will be seen as the greatest 20th century philosopher along with Russell and a few people like that.

I think of it as sort of very important, but in almost the opposite way, which was that philosophy... There’s two aspects of philosophy. There’s... there's the current new philosophy and then there’s the... the philosophers who are really historians of thought and so forth. And it seems to me that the first kind who were just wondering how things work are... they’re what produces the sciences. And as soon as you have a Galileo, then the philosophers who tried... then the Aristotles who tried to understand the world in... they tried to understand the world in terms of the concepts already there, maybe. And then, when they hit a stone wall and things don’t change, it might be a hundred years and then somebody appears who later we call a scientist who says, oh maybe there’s this and that. So, I... I see philos... I think maybe we’re talking about the philosophers on the scene trying to do something and the philosophers who are employed to teach people the old stuff. And of course they're both... they go... they both have the same name.

Right. So you... you think of, well what can I... how can I tell if something is true? And the usual way is to think of better reasons. And at some point you say, well I’ll roll the ball down the hill and see if it... see if it reaches a maximum speed or if it keeps going faster and faster. And of course, you’ll get the wrong answer. It’ll reach a maximum speed, but that's... but for the first minute it’s accelerating and you’d get Newton’s law. After a while, it’s going 80mph and never gets any faster because the air's and... I once was looking at an exhibit in the science museum downtown. And one of the features of the science museum is that it has... it has this tube that's very long, about four or five stories. It’s a glass tube about this big. And they drop a feather and a... stone; I forget what the other thing is. And of course, the stone comes down very quickly, or it's a ball... something. And then, the feather slowly drifts, and then they pump the air out of it and do it again and they both come down at the same time, but if you look very closely after they both come down, then you see little pieces of feathers still coming down very slowly. 'Cause there’s still some air in the thing.

He was an intuitive mathematician. Didn't... couldn’t do any real mathematics, but he guessed...  he was remarkably good at guessing... guessing what might be true. So, there were a few of us who hung around and proved that what he conjectured was right, but... but I have ghosts because of this. So, if I’m stuck on a problem, sometimes I imagine Andrew Gleason saying: 'Well, you’re wasting your time.  Try something else.' Or I have the... I used to hang around John Cage a little bit and... he’s one of my ghosts, I didn’t know him that well, but if I’m doing something, he’s saying: 'That’s too pretentious.' And sometimes I have the voice of McCulloch saying: 'You should... you should make that more grand. It's really... show why it’s important.' So, McCulloch is one of the people who watches me when I write and says that's... either, that’s too pretentious, or, not pretentious enough but... do you have people like that? I think I sometimes say... it's a joke between me and my friend Vitek, we say... he made this up. Which was: What would Feynman do? Exactly. For mathematics I say, what would Gleason do? And... that’s the advice I give students, but they don’t listen much. And that's... if you’re talking to someone like Feynman or Gleason who says something, what the students do is they take notes and write it down. And they try to remember what he said, but what you should say is, how did he think of that?  And then, you can go and ask him.

As a student, I once... was in a class with... I think it was... I’m trying to remember the professor; it may have been Edward Purcell. But it was... wait a minute. I don’t remember which mathematician it was, but one of the students said: 'Well I understand what one and two and three dimensions are, and… and… I see a little bit of what might be like in four dimensions, but you keep talking about n-dimensional spaces' – Hilbert space has infinite number of dimensions – And the student said: How do you think about these n-dimensional spaces?' The professor stood there for maybe a whole minute and he said: 'Well, you’re not thinking about it the right way. Just think of a particular n.' And then, he went back to... it's a great example of not being helpful. So, that was… I was a young student and I asked Andrew Gleason: 'How do you think of a high dimensional space?' He said: 'Well there are a lot of ways.' So here’s the Society of Mind idea. And he said: 'You can think of a bunch of vectors and just remember that each one is at right angles to all the others.' And then here’s another one, it's… say… whatever… whatever space you’re thinking of, think of it as a surface and ask, what's the solid thing that it’s the surface of? And, if you keep doing that, you keep going in dimensions. And he sort of sat there and mentioned six or seven of these. And I think most students have never had that experience of… of getting that kind of answer where instead of… instead of being told the right way... But the idea… you have to have a lot of vectors and just remember that this one is at right angles to all the others. Well, there’s no way you can imagine that, but… but you can have these little tricks. You can say, oh well I’ll make a… little bundle of this with the other two, that's no problem. I can’t do it with the other three, but who cares. So, that’s the lesson. If somebody says something smart, you should... how did he think of it? Not: How can I repeat that later?

Is this a thing that you need a special talent for or is it something that any child could do if somebody suggests it to them in the right… with a couple of examples of how to do it? Is there really a... do real people have a fixed level of intelligence or is it just the way the world is? Most people only discover 10 ways of thinking and other people just happen to discover oh, here’s the way you make a new way to think. So maybe being smart has just a couple of seeds that are a little different and it’s an accident that… that it gets evoked. I remember once when I was a child and I asked my father something and he said, 'Well I don't know, but I’ll find out.' And I had this image that... this… this expression, find out. And I imagined that when he went to work – whatever that is – in the day that there was some kind of place that he was and it had… it had a dungeon, like a well. Or maybe it was a library, but it’s underground. And if there’s something you want to know, you go down there and there’s a book that has the answer. So, finding out... I had this image that there was this... there was a particular thing you did that nobody had told me how to do. Of course, later was the problem... all these adults. Why don't they… why can’t they find things out? It was like that… that incident of building the 20ft high, or whatever it was, Tinkertoy tower. And the adults would come by in this lobby and say: 'How could a little child build something so big?' And I remember that my thought was, how could someone so big not understand that all you have to do is keep adding to it?

Yes, I remember there was one intelligence test that I took some time when I was a little child. And, this person would ask different questions and one of the questions was, there’s a round, circular field and there’s something lost in it. How would you find it? And one way would be to… I… to go to the center and spiral out, and… but I must have thought of that and... but I didn’t like that because you would cross your path where you came in. So, I said: 'Oh well, you could do it by going like this on one side back and forth and then you go around over to here and go back and forth this way and you come out' and... then later, the experimenter said: 'Well that wasn’t the best answer, you should do this…' and I realized that whoever had designed the… the answers didn’t realize that you should take a much more complicated path that keeps you from crossing yourself. So then, I wondered about... how intelligent are the people who make these tests? But, it’s nice that they put geometry in. A lot of tests are just verbal.

What about this... you did lots of interesting things at school. Like one of them was to do with chemistry. You had a teacher called Herbert Zim, I… I gather.
Wonder… yes. He was a… it turned out he was… later became a high official in the Museum of Natural History. But he was a teacher at the Fieldston School. And I remember that I had some idea of making something because I had read a chemistry book. Oh it was… it was to be ethyl mercaptan, which is two ethyl groups with a sulphur, I think. And it would be interesting to make because it was supposed to be… have a terrible odor, and if you made even a few milligrams, you could stink up the whole building. So, he said: 'Well how would you make it?' And I think I had read that well, you could first make ethyl chloride, which wasn’t so difficult. And then, you would do something that would replace the chlorines by the sulphur and... actually, I don’t remember how the synthesis worked, but this must have been fourth grade and it was quite clear what you had to do. And… Herbert Zim said: 'Well, that sounds pretty good, and well, why don’t you try it?' And he gave me a little room with the right equipment and even a vacuum hood to... so… but I had to make the ethyl alcohol first, so I had to ferment something with yeast or whatever it was. It was a long process, and then – I’ve forgotten the synthesis – but at some point then I made the alcohol into ethyl chloride and it instantly evaporated and all disappeared because it’s extremely volatile and… I’m sure Herbert Zim knew that I would never get to the last step in this process and stink up the whole school. Though I… I admire his understanding this plan, but no sooner did the ethyl chloride start to appear then it... you could see it vanish as soon as you opened the bottle or whatever it was. I… I don’t remember anything about this except the narrative but... but that was the great thing about that school. If you had some idea, some teacher might help you do it. This was the same school that Oppenheimer was at. That was the school he had been at a few years before. When I met him, we swapped a couple of stories, but I can’t remember any details. He had… he was now at the institute [sic]... so I didn’t know him during the... the atomic bomb days was before my time, but the… being at Princeton you could just... the Institute for Advanced Study is just walking distance and... So did you used to go down there and see him? A few times.

I don’t remember really what he was like very much 'cause… but I remember that he was very engaging and he really wanted to know what I was interested in and... and… then, he would say: 'Well why don’t you talk to von Neumann about that?' So he was… arranged... I think I mentioned that he got me to have lunch with Einstein and with Gödel also at different times. I didn’t appreciate how extraordinary this was because – as I said before – I grew up in this golden age of post war Harvard and Princeton, where all the greats... it seemed to me that all the great scientists I might ever want to meet were just down the hall or in the next building and... today, it’s again the same because of the Internet if you can get access. Because, after this gap of 30 years or so... right now, one can potentially email just about anybody on the surface of the planet, especially if your own name is a little well-known. Then, you can get through, so… so there must be some invisible networks of very influential people who… who are doing wonderful things because this communication system is around. And then, there are huge numbers of people who are getting involved in completely useless things because they’re so attractive and easy. The social networks are so easy to operate.

There was a wonderful man named Pitirim Sorokin and I took his course on History of Civilization or something. He was sort of... I’m told he’s… compares… compared to Spengler, who wrote this huge book on the decline of the West and... Sorokin had written this rather big book called Contemporary Sociological Theories, which sounds pretty boring, but each chapter is about some person who made a theory of history. So I forget what’s in it, but, you know, there'd be a chapter by Spengler and a chapter by HG Wells and just about every historian. Not by them... it would be Sorokin’s summary of… of Hegel’s theory of society and... I was fascinated by this man who knew all of the history of these theories of history and... we became friends and he played some music for me that I had never heard and... sort of like meeting Aldous Huxley, who I never met. But Sorokin was this scholar who had just… knew… knew more different things than anyone I’d ever met before. And, being at Harvard in those days... every classroom had such a person. Maybe they still do.
[Q] Amazing. Your contact with non-academic people... it was quite limited I suppose.
 I think so. It’s hard to... there are just so many days… hours in the day and days in the week. So… I don't… have almost no non-technical friends.

Well, it’s been... mostly it seems as though it’s a separate thing and... very often... it’s not all the time, but often when I’m doing something I’m also improvising with some tune in my head or some… trying to imagine some sequence of chords and... it’s hard to explain because there’s something wrong with it, the way I do these things. I can imagine a chord when my hands are open, but if... in other words, I suspect... and… I could actually go and get a brain scan, I suppose, to see, but I think that the music processes that I’m running frequently are in the motor cortex rather than the sensory cortex. So if my hands are closed, I can’t think of a piano piece. But if I go like this, now I can imagine… and I think what must distinguish musicians of some sorts – including me – from most other people is that... when I was a child, I was very much interested in the idea of trying to think of two tunes at once. And as you…  we all know, most popular music usually has one tune going on, it’s a song. And then, there’s a rhythm section and percussion and there are other things accompanying it, but there’s usually a main theme. However, in the… in the times of Palestrina and Bach and Beethoven, in the classical music, there was more concern with trying to have more than one thing going on and... the ultimate complication of that is in the traditional four part fugue where you’re running four things at once. And the question is, can anyone actually think four things at once, and it looks like Johann Bach and a few other composers were able to do that.  And quite a few modern composers are able to imagine several themes going on, but they’re not... well, I don’t know what they imagine, but I think somehow this unconscious or semi-conscious preoccupation with trying to think several musical things at once became a habit with me. I never got very good at it in the sense that I can’t… I can't think of three tunes at once. I can improvise it and fool musicians, but really by the time when I start to bring in a third idea, one of the other two starts to weaken and just become extra notes in a chord.

I find most people say, well it’s either this or that and I’m always inclined to look for a third thing. It’s not like Hegel’s dialectic and synthesis where you have two things and then you try to find a third thing that’s made of both but... I think I’m always... if somebody says, is it left or right, I’m always looking for a third way. And… of course, most of the time you can’t find one, but every now and then I get a new theory because I found that some community has gotten stuck making a di-stinction. The joke I made in that Society of Mind book was complaining that there’s no word – in English at least – for tri-stinction or trifference. Then I started to look at the world of children and we don’t give children triplets of things, there's… there's...  everyone has two shoes and two hands and two feet. And the interesting thing... if you look at child’s development of numbers, children get the idea of two when they’re two years old, very usually. And a three year old counts to three and then they suddenly take off and go further. But it… it’s a whole year for a young child to get from having words for two and words for three. I never made a scientific study of this but… it would be interesting if there were more toys that... like Lego is square, but Tinkertoy has triangles. I’d like to see more. Could you adapt Lego so that it had more triangular structures? Well then, it would be harder to make things at first, but easier later.
[Q] Did you teach yourself this way of thinking or is it just how you’re made up?
Well, I think the… the trying to think of several musical things at once is... I assumed that everyone did it and I don’t remember ever talking to anyone about it but... my father was always playing two part inventions and once in a while, he’d try to play some three part Bach thing, but usually he was just doing two but… but it seemed to me that that was the interesting music around and most… most popular music doesn’t have two things or one of them is a constantly repeating thing in the background and it’s not so interesting. I don’t remember ever talking to anyone about it. 
[Q] But you loved music. 
Hmm, I’m not sure what that means. I just did it. I liked counterpoint.

We spent a lot of time walking around Yerevan and talking about things, but he... he wasn’t interested in consciousness yet because I guess this must have been in 1960s.... late 1960s or 70s. I don't know. He was still interested in genes.

OK, in this book that I called The Emotion Machine – by mistake – I have a chapter on what’s called consciousness. And it seems to me that this is a very popular word and a very mysterious word. So many otherwise very smart people like to talk about the mystery of consciousness. What is this wonderful thing that we have and how could a machine have such a thing? Most people believe somehow it's... it couldn’t be artificially constructed because it’s so mysterious and so powerful. And I have three typical statements of... that people make... no one in particular. Consciousness is what give things... gives things meanings to us. Without it, we would not even know we had feelings. Consciousness makes us aware of ourselves; gives us our sense of identity and gives us the sense of being alive. Consciousness is what binds all our mental events together, unifies our past, present, and future into a continuous sense of experience. And I just have a little comment: Wow. Wouldn’t it be astonishing if any one power or force could endow us with all those abilities?

So we have many prominent scientists who say, we understand lots about biology and lots about the brain, but there’s one mystery that perhaps we’ll never solve because it seems so important. What is this wonderful thing called consciousness? And… wouldn’t it be astonishing if there were such a thing? Well, I don’t think there is any such thing.  And, so here’s an example of why I think that’s the wrong question to ask. So in this book... in the middle of the book I tell a little story and then the story keeps coming back because it has so many questions. It raises so many questions you want to answer.
So, I imagine this young lady named Joan. And she is…Joan is starting to cross the street on the way to deliver her final report. While thinking about what to say at the meeting, she hears a sound and turns her head. And she sees a quickly oncoming car. She doesn’t know whether to try to cross or go back, but she’s worried about arriving late, so she runs across the street quickly. Later, she remembers that she had hurt her knee the previous day and realizes, that if my knee had failed, I could have been killed. Wasn’t that a terrible thing to… a risk to take? If I had been killed, what would my friends have thought of me? So, she’s imagining being ashamed of having been killed for no particularly good reason. So you might ask, what’s all this about? And most people would say, Joan was conscious of making this decision and of many of these aspects, but let’s look at what Joan actually did. First, she quickly reacted to the sound. Well, that doesn’t need consciousness. You can easily make a machine react to a sound. She recognized it as being a sound. Well what does that mean? She classified it as the sound of a car. Attention: she noticed some things rather than others. Indecision: she wondered whether to cross or retreat. Imagining: she imagined two possible futures. Decision: she chose one of several alternative actions. Planning: she constructed a multi-step action plan. Reconsidering: later, she reconsidered it.
Well, I just mentioned about 10 things, but I have 20 more. Learning: she described the situation and stored those descriptions away. She retrieved descriptions of prior events.  She described her body’s condition. She constructed some verbal representations. She arranged these into story-like structures. She changed her goals… priorities. Apprehension: she was uneasy about arriving late. She thought about what she’d recently done. She reflected on what she had thought about. She imagined other persons’ thoughts. She evaluated the… the right and wrongness of her decision. She characterized her mental condition. She made models of her own thinking and she regarded herself as an entity. Well, all these 20 or 30 things are different kinds of processes and we all use the same word, consciousness. And then, we ask, what is this mysterious thing called consciousness? And it seems to me that a large fraction of our philosophers and psychologists have gotten stuck by thinking that there’s a single explanation or a single entity or a single process that does all those things. And that’s crazy. So, in the Society of Mind theory that I… Papert and I developed about 30 years ago, the idea was that... well, maybe in the brain there are 100 or 200 or heaven knows how many different processes that do different things. And they’re all involved with different levels of analysis and different kinds of patterns and different kinds of representations and so forth.

It’s 400 million years since the first multi-cellular animals appeared. And then we have 100 million years of… of evolving toward… toward fish and getting vertebrae and things like that. Then, it’s 100 million more years of the fish becoming amphibians and moving on to the land and also – of course – continuing to evolve in the sea. And then, it’s 100 million years of... then, they’re amphibians. And it’s 100 million years of being reptiles and land-living animals. And… and then, in the last 100 million years, the mammals started to appear. The warm-blooded animals with... and all through this process, the nervous system was becoming more elaborate and complicated and having more… different parts of the spinal cord and brain. And in the last five or six million years, the… our ancestors the orangutan and the… the gorilla and the… baboons and the chimpanzees... and we split from the chimpanzees pretty much in the last 50 or 100… 50 million years or less. And all of this involved developing more and more complicated structures in the brain. And so, to me it’s sort of pathetic that people say, and the wonderful thing is that it suddenly became conscious and this new thing appeared instead of 30 different things. And so I’m sort of ashamed of my colleagues who say, well this is a philosophically difficult problem. How could… what is consciousness? It’s not matter, it’s not material. Where did it come from? It’s kind of… inexplicable spirit. Surely, it’s something that cannot be explained scientifically. The answer is, they're… they’re absolutely right because there isn’t any such thing.

Conscious is the word we use for all the different things we don’t yet understand about the mind. And the sooner we get rid of such words... now, these words are very useful and we can’t get rid of them. For example, suppose somebody kills somebody else by accident. They didn’t intend to. Well then there’s no use punishing them, unless they’re prone to do it all the time and in that case you have to isolate them. But we need the word consciousness for ethical reasons, for fooling people into behaving the way we want. So we make the distinction between a decision somebody makes because of a complicated process we can’t understand, but which we can interfere with. So, you see, it’s a legal and it's a social and an economic term that we really need. How do we control people? Well, we have to distinguish between the things that they do because of using these 30 or 40 different mental processes because we can teach them not to or we can warn them not to. Other kinds of processes that are lower level and involve less machinery are harder to interfere with. And so, legally, we… there’s no point in punishing them for things that – in some sense – weren’t intentional, that they couldn’t control and... so I’m saying that the reason the word consciousness is so popular is not because it has a scientific importance. It’s because it has a political and social and economic importance. And I’m ashamed of my colleagues who don’t understand the difference between cause and opportunity. It's... it's… maybe we don’t have the right words to distinguish between the ways we have… we need for describing people for different purposes.

Now, so much for consciousness, now there’s another problem that I see as almost the same that concerns a lot of modern philosophers. And that is... they have a special word called qualia. And they use this wonderful word, qualia, to refer to the character of sensations. So for example, we can say an object is red or another object is green. Now, about 10% of people don’t make that distinction very well because – particularly men – because red green color blindness is rather more common that most people who don’t have it realize. But…. but so… that’s an example of something. But what many believe or like to think is that there’s something very simple, clear, and absolute about the qualia of a sensation. Like something hurts or something is red or something is hard or soft. Well that’s… not a bad example. And so, they say, well how do you explain the existence of characteristics like certain colors and certain sensations and…? How do you explain the difference between pain and pleasure and…? Why… what are these qualities that seem so simple, absolute, and clear that you can’t explain them? And the answer is, you can’t explain them because you’re too lazy. There aren’t any such things. The process by which the nervous system discovers or identifies something as red is extremely complicated.
For example, if you have a visual system where there’s one color here and another color here, then it can be very difficult – if there’s nothing else in the scene – to identify those colors because all you can see is the difference. And in fact, Edwin Land discovered that in many cases you can’t identify... you don’t have the sensation of a definite color unless there are three regions that are optically… that have optically different colors coming together. If there are just two regions, then your sense of what color the two regions are can keep changing or can be different from what it would be if… if you put it on a table with a third color. So, it turns out that these qualia are not absolute. As far as I can see, they depend on all sorts of complicated processes running in between. So, the sense that you say something is pink or something is orange seems very immediate, but that’s because... let’s take another example. How do you… why did you say the word orange? Everybody knows the words they just said because one of the 30 or 40 meanings of that word, consciousness, is remembering what you’ve recently done, but nobody knows how you… what process caused you to emit the word orange. Is there just a connection between the color receptor in the retina and a thing in your speech centre that produces the word orange? No, there’s no reason to think anything is that simple. There's probably 15 or 20 different layers of processes.

So when these philosophers talk about qualia, they’re saying that they think that in peoples' mind there are very simple and therefore incomprehensible relationships between the external world and the ideas you have about the world. What I’m saying is that usually there are probably 10 or 20 different layers of processing and perhaps 30 or 40 or 100 different processes involved in producing the final reaction and the new state of mind that comes from this. So, the idea that there are qualia is sort of like the… the people who talk about those things remind me of the Greek philosophers who started the atomic theory with a rather good idea that there was earth, air, water, and fire; namely, solids, liquids, gases, and... I forget what... plasmas. I don't know. And they got stuck at this very simple level. So, there’s some well known scientists… public philosophers who talk about qualia and say that this is the deepest problem about how the mind works and the most profound. And I think that’s complete nonsense. If you take the word consciousness or the word qualia or whatever they’re saying... then probably the way to deal with that is to break it into three, 10, or maybe even 30 different theories and find out how they’re interlocking and... and how they work. When a person says something, they know what they’ve said because they can remember what’s happened, and that’s a complicated process itself, but they can’t tell you how they... when you saw something orange, how did you choose the word orange instead of the word dog or round thing? And if you ask somebody that, they say, oh… it must be a direct connection between the stimulus and the production of the word or... Well of course there's not. It’s very complicated. It takes a child 18 months before they start making the… assigning the right words and heaven knows how many layers of complicated processes there are. But as far as the reflective aspect of so-called consciousness, you just saw it and you said what it was. It seems simple and immediate just like if you tell a horse to... never mind. If you tell somebody to do something and they do it, you don’t say, isn’t that a miracle? How come they know exactly what you mean? The answer is, they don’t know exactly what you meant and neither do you. And there are 30 or 40 things and... it’s all wonderfully complicated.

The important thing… the important difference between psychology and physics is that evolution intervened. And… so, the properties of a person don’t depend on simple physical things, but they depend on the intervention of a computer; of an enormously complicated intermediary system with many parts. Therefore… I know I once counted in this big neurology book... I went through the index and there were 400 words referring to parts of the brain. The caudate nucleus and the lateral this or that... the frontal that… there are all sorts of geometric descriptions of these objects.  And… so, when something happens in the psychological world, it might be because of the interaction of 15 different kinds of computers that evolved over that wonderful 400 million years between the time we were yeast and the time we were mutated chimpanzees. A lot of changes happened. And so, the laws of psychology are not like the laws of physics. You’re not going to find seven principles that explain almost everything. You’re going to find 7,000 principles that come from the interactions of 20,000 or 30,000 genes or... that number keeps varying because people find little things in the genome that affect others and… the number of… we know that the number of developmental influences is in the tens of thousands in any case. And so, any explanation of what a biological organism does is likely to involve incredibly complicated explanations whereas if you take a simple physical or if you take a typical non-computational physical system, then there’s a chance that you can find a… a pretty good simple explanation for some aspects of what it does. So, psychology is... it's… maybe we shouldn’t call it a science because we like to think of sciences as very orderly and highly evolved systems… are just… the history of evolution is a history of immensely funny accidents. Almost everything could have happened some other way and... so there’s not much significance in the particular way any particular biological system works. It depends on the history of what happened in the millions of years before that thing evolved.

There was this wonderful incident of a meeting in England where I first met Seymour Papert, with whom I worked continuously for the next 30 years. And we both had the… had presented pretty much the same mathematical theory of how a... how a certain neural network could learn. And it was reinforcing the connections between synapses in accordance with such and such set of rules. And that alone… that very simple model shows how a machine could learn certain interesting classes of behaviors; not all, but some and... we worked for maybe 10 or 15 years on trying to find to what extent what kinds of phenomena could be learnt by a simulated neural network using this particular kind of learning by reward and… and extinction by not so much punishment, but just by not rewarding it. And we discovered that those particular kinds of neural networks could learn all sorts of interesting and useful things.

But one thing that they couldn’t learn was to count the number of objects in a picture. It's… just turned out that that was... because of rather obscure mathematical features of the concept of counting, that a reinforcement neural network of the type that we… many people were interested in just couldn’t do that. In order to make a machine that can count, you need a machine that has internal loops inside of circular causality and you can’t make a machine that just passes signals from one layer to another, processing them in a kind of linear form. Well, it took us several years to prove that for certain neural networks and… although we published all of this around 1970, there’s still some more things to prove about it. And for some reason, nobody has developed... this was a field called perceptrons. And some rumor started to spread that by reinforcing the machine in a slightly different way, it could overcome these limitations. So, the whole world of neural network people seem to believe that this problem has been escaped and that we were wrong in saying that there was no way these machines could learn that. And because of this false rumor, there hasn’t been any further progress in understanding the limitations of neural networks with many layers, but without loops. So, one reason I’m annoyed at my colleagues in… in this particular field of psychological theories is that they believe these rumors without actually looking at the original problem and seeing that it’s still there.

The perceptrons did some things that were quite useful and remarkable. And then, there were some things that seemed remarkable, but turned out to be accidents. Like I had a friend in Italy who had a perceptron that looked at a visual... it had visual inputs. So, he… he had scores of music written by Bach of chorales and he had scores of chorales written by music students at the local conservatory. And he had a perceptron – a big machine – that looked at these and those and tried to distinguish between them. And he was able to train it to distinguish between the masterpieces by Bach and the pretty good chorales by the conservatory students. Well, so, he showed us this data and I was looking through it and what I discovered was that in the lower left hand corner of each page, one of the sets of data had single whole notes. And I think the ones by the students usually had four quarter notes. So that, in fact, it was possible to distinguish between these two classes of... of pieces of music just by looking at the lower left… lower right hand corner of the page. So, I told this to the… to our scientist friend and he went through the data and he said: 'You guessed right. That’s… that's how it happened to make that distinction.' We thought it was very funny. A similar thing happened here in the United States at one of our research institutions. Where a perceptron had been trained to distinguish between – this was for military purposes – it could… it was looking at a scene of a forest in which there were camouflaged tanks in one picture and no camouflaged tanks in the other. And the perceptron – after a little training – got… made a 100% correct distinction between these two different sets of photographs. Then they were embarrassed a few hours later to discover that the two rolls of film had been developed differently. And so these pictures were just a little darker than all of these pictures and the perceptron was just measuring the total amount of light in the scene. But it was very clever of the perceptron to find some way of making the distinction.

One problem with my field is that people publish experiments where the machine does succeed in learn… apparently learning something – maybe really learning, perhaps – but they don’t publish failures when they say: 'Well, we tried to get this machine to distinguish between cats and dogs and we had 4,000 pictures and it didn’t do much better than chance.' We never see papers published, or hardly ever. And so, there’s something a little bit wrong. In physics, you win a Nobel Prize if you can show that some old theory isn’t quite right. In artificial intelligence, you might lose the company that’s paying for your research if you publish a failure. And so that’s another reason why we need more institutions that are not based on quick payoffs.

Shannon’s first publication was about how you could use relays to compute all sorts of different functions or make machines that would do a lot of interesting things. And this was the first paper that systematically made a mathematical or a logical representation of how relays could be assembled into... of what collections of relays could do if they were connected in various ways. And to this day, people use that particular way to represent electrical circuits and theories and... I remember once looking up the paper in… in the journal and the paper next to it was reporting the invention of the fluorescent light bulb. So, here you have... in the middle of this rather thick Institute of Radio Engineers magazine… electrical engineering magazine, there’s the paper by Claude Shannon which – in a sense – led to the modern theory of computers and the next article was how to make this light bulb which eventually replaced most of the light bulbs in the world. I wonder if anybody... I’ve always been meaning to go back and look this up and see if there were three or four other world changing articles in that very same issue.

Claude Shannon had been working at Bell Labs in Murray Hill, New Jersey. And I forget how we became acquainted, but in the summer of 1952, when I was in the middle of graduate school at Princeton in fact, we got invited – John McCarthy and I, who was also a graduate student – to spend the summer working with Claude Shannon on theories of computation and things like that. This is before there were hard… there were hardly any computers in the world in 1952, but somehow, that connection was made. And we hit it off because both Shannon and I were addicted to making interesting new mechanical devices. And we both had Erector sets and Meccano sets, which enable a child to build pretty complicated machinery in a few minutes. So, I think the reason that Shannon and I got along so well was that we had pretty similar mathematical interests, but we were also interested in mechanical gadgets and... I remember one day, we were trying… I was trying to assemble something by pushing a wire into something and I couldn’t figure out how to get it to go through all that… little turns and things. Shannon said: 'That shouldn’t be hard, it’s as easy as pushing a string.' And it sort of characterizes this problem in a way that has always stuck in my mind. For… some problems are hard because they’re terribly complicated. Some problems are hard just because it’s not in the nature of the thing that it can be done at all.

While I was there... I was only there a few weeks and one of the nice things about Bell Laboratories... again, we have to understand that at that time, the telephone company was a monopoly and it had the world at its fingertips. Everyone in the world had a telephone, even in those days. And there was no competition because... I don’t know what… I don't know what because… but that means that when we arrived at that place, somebody said: 'Don’t work on anything that won’t take 30 or 40 years.' You don’t hear that anymore and... one of the nice features of Bell Laboratories... it had a stockroom which had everything you could want. And I was once walking through the stockroom and I saw the balls of ball bearings and... there were tiny little balls that were so small, you could hardly see them. And there were… they went up to balls that were 2 or 3in in diameter that were pretty hard to lift. So, I got this idea. And I… I knew that if a large ball rolls into the small ball, then the small ball will bounce off twice as fast. It’s an interesting bit of physics coming from Newton’s laws. Imagine that you have this big ball and a small ball drops on it and hits it with a certain velocity, V. Then, it'll bounce off… and bounce off with velocity minus V. So if it’s going 20mph this way, it will be going 20mph when it comes off, or a little less. 'Cause the big ball… the big ball won’t move much. So all of a sudden, 20mph has turned into a change of 40mph.
Well, it turns out that the same thing will work if the little ball is standing still and the big ball is moving. So suppose you have a little ball here and a big ball hits it, guess… and its 20 mph. Guess how fast the little ball will go? 40, or 39. Well, if they’re all… if they're not quite so different, what will happen is the little... if the big ball and... you have a slightly smaller one, it will go somewhat faster. And I realized that if you made a whole chain of balls, like 2in and 1¾  and 1½ and so forth, that the more balls you have – and if they’re smoothly tapered – the greater the gain in speed will be. So, I got one of every size and I put them in a rack and I rolled this great big ball into this next one. And the little ball disappeared and went through the window of… across the hall. And that was the office of another great scientist named Hendrik Bode, who discovered some important new phenomenon in electrical engineering. Luckily, he wasn’t there. I don't think it would have killed anyone but...

In this period of three or four weeks that I was at Bell Labs, Shannon and I must have invented 10 or 20 different gadgets. One of the gadgets that I invented – which was really perhaps the most stupid machine of all – was I described a little box with a switch on it. And when…when you flip the switch on, a hand comes out of the box and turns the switch off. So, that was nice. And Claude had a machine shop and he actually built one and… somehow this machine got a lot of publicity because it was... most people thought that… that was perhaps the most useless machine ever made so far. Anyway, we collaborated on all sorts of theories, but mostly on just thinking of fantastic new gadgets.

Yes I think in 1950 or so, Claude Shannon wrote a… an essay about how to make a machine to play chess. And in retrospect, it’s quite obvious what you have to do. You have to generate possible moves and you have to decide which of two moves is better, but you can’t decide that easily except to see what other moves… subsequent moves will result… could result from it. And this was a beautiful paper describing exactly how to make such a machine. There was in fact a similar essay, which hadn’t been widely published, by Alan Turing, the… the scientist in England who also made major discoveries about the foundations of computation. And… I don't know what to say. Computers were still too slow at the time that Shannon wrote this paper to make a chess playing machine but which – now we’re talking about the 1950s – but by 1960 the computers were fast enough and we… we and other people wrote computer programs that did more or less what Shannon had described. And we had the first… the first chess playing machine – I think – was made by Allen Newell and Herbert Simon. And the computer was still too small for a whole chessboard, so it played a sort of… miniature kind of chess on a 6x6 board instead of an 8x8 board. And it beat a secretary who had been taught the rules of chess in the hour previous to this match. So, it was the first machine victory, but I don't think anyone took it very seriously. And then, a couple of years later, John McCarthy invented a strategy which is – for various reasons – is called alpha beta. Which makes the classical chess machine much more efficient and using this trick, by 1960 the simulated chess machines were starting to beat really good players, here and there and... no machine became chess champion of the world until the 1990s, but in the 1960s a… program to play checkers beat the state champion of checkers of Connecticut, a… a large state nearby Massachusetts here. And that was a notable event because no machine had beat a champion player of… at anything before that time.

I think the result of the machines being good at chess and checkers and... eventually, a backgammon machine program developed at Carnegie Mellon Institute by a researcher named Hans Berliner. It beat the world champion at backgammon, for what that’s worth, but I don't think any of these had any serious effect on… on the more general goal of making a machine that had something like human general intelligence: the resourcefulness… ability to solve all sorts of different kinds of problems. The trouble with these particular games is that you discover a few tricks that… at which machines are better than people because the machine is faster at doing relatively simple things than the person and... so, defeating a person in certain activities... it’s almost like a steam shovel and a person with… with an ice pick. If there’s no real difference except speed, then one isn’t very impressed if a machine... speed or strength. And these chess machines were not particularly clever in any important sense. They couldn’t solve other kinds of problems.

I had an extraordinary run of good fortune – I think I’ve mentioned this before – when I was in college of just meeting people who knew a lot about exactly what I happened to be interested in at the moment. And as I mentioned earlier, I was very strongly influenced by my accidental encounter with the great book by Nicolas Rashevsky called Mathematical Biophysics. I just happened to find it in a library and it had articles about applying mathematics to different aspects of biology. There would be a chapter on... how does a cell divide? Or how… what kind of physical system could cause a cell to contract at the equator and break into two parts without spilling all its contents? Just lovely little questions that any child could see... that’s… that's a question. How could it work? How could a nerve fiber conduct a signal? Chemical reactions might seem too slow to go at 100 meters per second so… or 10 meters per second or whatever it is. What’s the trick? This book was full of different applications of ordinary mathematics to biological problems. I’ve never seen anything like that. And one of them was about learning machines and neurons by McCulloch and Pitts.

I started to read about theories of learning and the prominent scientist of the greatest reputation was Burrhus F Skinner, Burrhus Fred Skinner – we all called him Fred – who had written a book in the… I don't know, 1930s or '40s about how animals could be trained to learn by giving punishment and reward and… particularly extinction, which is just not rewarding them. Which is more… if you punish an animal for making a response you don’t like, that will quickly make the response disappear, but shortly after you go away it'll… it'll start trying it again and... it turns out that if you extinguish a response you don’t like by punishing an animal, then when you disappear, the animal may act as though he’s learnt to do it even more in the meantime. It’s a phenomenon that Skinner discovered. Maybe it hadn’t been known. So, the way to permanently extinguish a reaction... it’s better simply to ignore it when the animal does it and not give it a reward. Anyway, he discovered a large number of things like that. And of course, earlier scientists like Watson in… in America and Pavlov in Russia had discovered many things like that in… around the turn of the century, 1900. And lots of people had followed Pavlov and done experiments with dogs and rats and things and pigeons, but Skinner was the first person to move this from a sort of anecdotal science to a very systematic science. And so, he had done lots of research with rats and pigeons where they’re in a box, which is completely isolated from the world with no sound and no light except for the stimuli he wanted. And so, he’s doing scientific experiments with very good controls whereas we have a movie of Pavlov training dogs. And it’s just in a big laboratory and there are lots of other dogs in cages around and… it’s not so systematic.

So anyway, I admired Skinner for having such clear theories and systematic ones. And… and I was good at electronics, so I hung around that laboratory and we became friends and he got me to help wire some cages. So, I really had this experience of helping to set up these experiments, although I had concluded that they were too simple minded and you would never understand how people would learn things like language based on… on that sort of thing and... but the… the main thing was that he… he was happy to discuss these things and he introduced me to the other two young assistant professors who were major figures in the rest of my life: George Miller, who was perhaps the most important scientist starting modern cognitive computational psychology, and Joseph Licklider, who was… had worked on theories of how hearing worked, but was beginning to envision the computer of the future and eventually who went to Washington and organized the Advanced Research Project Agency to fund basic research on artificial intelligence and psychology. In fact, he got the funding for the young people who created the first… the ARPANET, which became the Internet, which became… the new world. So, Skinner was the first person I had met who brought me into that environment. And… so we were friends and we… we did electronics together and so forth, but he didn't have… he was a behaviorist and he didn’t have… he didn't like making theories of what could happen in the brain to cause these things to work. But it was Miller and Licklider and this next generation who were interested in making theories. What could be in the brain that would learn that way? And is this the main way of learning or is this the way fish learn, but not people? And Skinner tried to argue that higher level learning was just more and more conditioning on conditioning rather than involving other new processes. But… but we remained friends for years. Again, it was… I inherited this golden age by just miraculously encountering just the right people at the right time.

Well, I was talking about being a young student, hanging around these wonderful psychologists in the basement of the Memorial Hall building at Harvard. That’s where the psychology department was in those years. And there was Skinner at this end and in the middle there was George Miller and Joe Licklider. And over here, there was Walter Rosenblith, who was doing pioneering experiments on the nervous system of cats. So, Rosenblith, who became provost at MIT eventually... an interesting move from Harvard. He was doing some of the early biology of the mammalian brain. But now, at some point... this is just a funny story.  I can’t remember if it was Skinner himself or Licklider, but one of them was... decided they wanted to study whether a rat would learn by... to extinguish a response by being given a little electrical shock when it pressed the wrong button. And the problem was how to give the rat this little shock. And so, I think it was Licklider, but the idea was... of course, the rat is in the metal screening cage of wire... wire mesh. So, Licklider went to a lot of trouble to build a cage where every other wire was insulated from... all the wires are insulated. And you would put a positive electric current on... electric voltage on the odd wires and a negative one on the even wires. So, wherever the rat was with its four feet, it would probably get a shock if you put this pulse of electricity into the cage. And in fact, it sort of worked and when the rat pressed the wrong button, you could give it a shock. And they set up some system for... to make it learn something. But the next day, now the rat was going through and eating all the food and never getting a shock. And when we looked closely, we discovered that this rat had managed to learn to walk only on the even... this rat was going all over the cage, but it was managing to only step on the even numbered wires and... I concluded that in order for this to happen, we really should try to understand this phenomenon where the rat had learnt something incredibly more complicated and difficult than whatever the experiment was designed to do. But... nothing ever came of that and they just gave up and didn’t do that experiment anymore.

One of the first things that happened was I was in... having read about McCulloch and Pitts and starting to read about the nervous system, I found a laboratory where people were working on nerves. And this was run by a professor named John Welsh at Harvard. And... it’s a nice story because the Harvard Biological Laboratory is a huge building and this building was new at this time. We’re talking about 1949 or... it must have been '48 or '49. So, the building was two thirds finished and almost empty. So, I encountered this Professor Welsh and said I was interested in learning more about nerves and things like that. And he said: 'Well, we do a lot of experiments with invertebrate nervous systems and we’re... we're interested in how the crayfish claw works.' It’s a little arm. The joints of the arm are tetrahedra. They’re sort of hinged this way and then at right angles. The next hinge is this way and the next one is this way. And that’s the anatomy of the mechanical arm I built that you see in the MIT Museum today. It's... the joints are modelled on the joints of the crayfish claw. But anyway, Dr Welsh said: 'The interesting thing about this crayfish claw is that it’s so easy to get at the nerves because...' he took out a crayfish and he said: 'You just break the limb off like this and pull it just like this and there’s this little transparent thread sticking out.' And it’s right there and you can see it with the naked eye and it consists of five nerves. And with a magnifying glass you can... and a needle, you can separate the five nerves. And one of them is an inhibitor nerve, so that if you stimulate with this nerve, the claw will close. Or, if you stimulate another nerve, it’ll move here but... you find the one that causes the claw to open and close. And then if you stimulate this other one, which is smaller, that’s the inhibitor nerve, it’ll open again even... so, I arranged a set of five switches and batteries or something – no, they must have been AC things – so that I could mount this crayfish claw here and the five nerves are connected to these five switches and... actually, I had it on potentiometers so you could vary the signal.
And after a couple of days, I got this thing arranged so I could get the crayfish claw to reach down and pick up a pencil and pick it up again. And... John Welsh called all the other biologists in and said: 'Look at this' and...  I was interested that they were so interested because... it seemed like such an obvious thing to do. So, he also... and you could get new crayfish every day because somebody drove up to some lake and... nearby and got them. And then, he also had me do some experiments with turtle hearts. And I don’t remember what these experiments were, but the main thing is that if you irrigate these things with cold water... these invertebrates are wonderful because these are cold blooded animals. And the turtle heart will work for a week if you just run Ringer’s solution through it and you can do really complicated experiments with almost no equipment.

The fascinating thing was that because this building was still being constructed and half-empty, what I got was this room with some equipment. And next to it was a darkroom for developing photographs and all sorts of facilities and... here I am, a... a lowly undergraduate with more equipment than... than most professors have. Again it's just... when I look back on this, it just seems like an endless...endless sequence of lucky breaks.

George Miller and Joe Licklider gave me this laboratory in the basement of Memorial Hall. And then, it must have been in 1949, I was working on this theory of making a reinforcement based... a Skinner based reinforcement learning theory, except it would have a random network of neurons and a way of changing the simulated synapses with little motors and potentiometers and things like that. So I designed this machine, which would be a randomly wired neural network learning machine. And then I... graduated, did a little thesis in mathematics about fixed point theorems on spheres that I mentioned before, and went to Princeton, but in the meantime I wrote a little proposal to George Miller about the theory of this neural network learning machine and maybe we should build one and see how it worked. And then... it must have been the summer of 1952. When... one summer was at Bell Labs and one was... [sic] so this must have been the summer of 1951 and George Miller got some money to build this machine from the Air Force Office of Scientific Research, which was a laboratory somewhere west of Boston. And he had convinced some colonel or bureaucrat or something to get a few thousand dollars, which was an immense amount of money in those days. And... I built... oh and I... and I had a friend, another graduate student at Princeton in physics named Dean Edmond... Edmonds, who was good at electronics. Both of us were, but he was... he knew even more about electronics than I did. And he offered to help build this thing.

We designed this thing and there were... the machine was a rack of 40 of these. So it was about as big as a grand piano and full of racks of equipment. And here is a machine that has a memory, which is the probability that if a signal comes into one of these inputs, another signal will come out the output. And the probability that will happen goes from zero – if this volume control is turned down – to one if it’s turned all the way up. And then, there’s a certain probability that the signal will get through. If the signal gets through, this capacitor remembers that for a few seconds. So that’s... that's the short term memory of the neuron. And then, if you reward the thing for what it’s done... there are 40 neurons, maybe 20 of them conducted impulses and something happened which you liked, typically. Then, you would press a button to reward the... this animal, which is the size of a grand piano. And a big motor starts and there... there’s a chain that goes to all 40 of these potentiometers. And... but in between that, there’s a magnetic clutch. So, if this conduct... if this neuron actually transmitted an impulse and this capacitor remembers it, then this clutch will be engaged. And when the big chain moves through all of these things, then the ones that have recently fired will... will move a little bit. And the amount that it moves will depend on how long ago it fired because the charge on this capacitor is time dependent and... once you’ve charged the capacitor, the current goes through this resistor and... and drains out. So, this is a little short term memory of what recently happened and this is the long term memory. So, I could make a simulated rat out of 40 of these. It had a big plug board and you just wire them all in random... this thing connected to that and so forth. And it could learn. It could learn and we built a maze which was a copy of a maze that Shannon had built. Shannon built a machine that learnt to run through a maze not using a nervous system, but using relays. And I sort of copied that and this one also could learn slowly. But it...  it learnt some things and it couldn’t learn some other things. And no sooner was the machine finished and we ran some experiments with it then I concluded that this theory wasn’t good enough.

In fact, we loaned the big machine to some students at Dartmouth. And it... so, it disappeared up to Dartmouth and about 10 years later, I thought it would be nice to collect it and see if it still worked, but no one could find it. So, this is the only trace – along with some photographs – of this machine, which was called the SNARC: Stochastic Neural Analogue Reinforcement Computer or something. Anyway, then I did my PhD thesis on theories of how to make a smarter machine which didn’t just make responses at random, but would look ahead and predict what would happen and compare different predictions and... sort of... that eventually turned into my six layer... Freud’s theory had three layers of this... this id, which is a very elementary kind of conditioned reflex learning machine and a superego, which is a self conscious high level machine; and the ego, which – in Freud’s view – is a rather passive process, which negotiates between ambitious goals that you want to do and primitive goals that... or fears or things that you have to do or... or try to prevent. Freud had wonderful... wonderful cognitive theories and... I had six layers instead of just three and that was... that came out in the Society of Mind book in the 1980s. But in the 1950s I worked on this... these reinforcement theories, which were inspired by the early theories of Skinner and... and those pioneers.

I wrote a little biography of the confocal microscope. And... the... the output was a radar screen. So, it took about 30... 15 or 20 seconds to paint this picture and I had a rather large screen and the machine had rather low resolution. So, the picture looked very blurry. And many years later I realised, if I had had a tiny screen, then the picture would have looked very sharp. And it was probably... of course, with the large screen you could see everything it had, which wasn’t so much and later I realised that the thing might have caught on if the... if the image had just looked sharper. Of course, it would have had slightly less information the smaller it was, but it would look like it had more. So, such is history. I just didn’t think of... of the presentation as being important. So, everybody admired it, but nobody said, I want one.

Nicholas Negroponte had a laboratory with a strange title; it was called the Architecture Machine. And... I think maybe this was because Nicholas’s... PhD thesis was based on a really remarkably humorous and interesting idea that... I don't know if I can explain it. It was a box that had a lot of guinea pigs and a lot of little wooden blocks. And in the course of their activities, the guinea pigs would run into the blocks and push them around and... Nicholas had some system, which would record – maybe just photographs – where the blocks were each day. And... he arranged some kind of plotting machine, which would... I think we may have to erase all this, but he had this machine that would observe that a block had been moved and reach down and move it back. And then, he would record whether the guinea pigs paid any attention to this or appeared to have any intentions about where they wanted the block to be. But I... I give up. I can’t remember the purpose, but anyway, that was called the Architecture Machine at first. And then, he had more students and Nicholas was incredibly productive at... at the beginning of computer science, of... thinking of things that no one else imagined computers doing.

Now, Papert and I had run the Media Lab for some years and then, Papert became more interested in applying his theories of education to actual children. So, for a period, the... the research on children separated from the artificial intelligence lab. And of course, if you’re teaching young children you need different facilities. The... the building codes have to be different so that there are more exits in case of fire and blah, blah... all sorts of restrictions. And as Nicholas’s Architecture Machine laboratory – which was in the architecture department – began to expand because of... the importance of computers in the 1950s was growing and growing and growing, he decided to expand and start the new laboratory – to be called the Media Laboratory – and got Seymour and I and a few other professors to... to be the pioneers of that.

Seymour Papert and I had worked on earlier versions of this society of mind theory and it was gradually shaping for maybe 10 years. And then, he began to get more involved with and spending more time developing the Logo language and getting children to use it and teaching teachers to train... to develop projects. And that was going well. And Seymour had less and less time to work on the basic cognitive theory and more and more need to develop the... his concepts of education. So, he started producing a series of papers and books about what some people call constructivism, the idea that in order to learn about things, children should actually be making things and building them and using them and trying to see what’s good and bad about them, whereas in regular school you’re being taught about things and being asked to write about them. But you’re not being asked to build things and do things and do experiments and so forth. So, constructivism is now a sort of key word all over the world.

We stopped writing together so much and I went on working on the... what became the chapters of The Society of Mind. So, in that period I had fewer students, but still a small number of very good ones who did work on some of these theories. And this happened in the environment of the Media Lab, but in effect I had stopped building robots and inventing gadgets and was spending more time trying to develop the successive levels of this theory. And so, The Society of Mind has this 300 pages of... each describing a few... couple of ideas and connections with others and... that was a... that happened in the early years of being at the Media Lab. A lot of these ideas – of course – developed by... by teaching things in a course. So every year I would give a course which involved lectures on the current state of ideas about these different topics. And the great thing was that there were no exams in this course, but the students had to write papers; short papers and then a longer paper toward the end of the term. And then, I usually got a few of the older... ex-students or graduate students to... to read those papers and summarise them and sort of grade them, but the main thing was that this was a... almost a decade of getting people to do thought experiments or... on... on developing these ideas. So, in the early years of the artificial intelligence lab, I was directly involved in most of the actual experiments and projects for programming computers to do things. And then this period, I’m working on these hundreds of small theories and trying to fit them together and getting students to write little essays about that. And that was most of the years that I was at the Media Lab.

Well, as I’ve said in many ways in... in this long career, I’ve had many... many great things happen that... that might not have happened. And I... I sort of see these as maybe a dozen strokes of great luck. Like... I never mentioned much about working with Patrick Winston, but when McCarthy and I started the AI laboratory, we had a wonderful situation where – as I said – both of us had ideas about what to do and both of us thought about different things, but we thought pretty much the same way, so it was only necessary to exchange a few words once in a while when something important came up or some decision had to be made. And, I've mentioned that I always loved being a co-director because if you could find another person who was so much like you, that you could go away and be sure the right decision would be made. Anyway, it was much better than having the weight of responsibility on you all the time. I can’t imagine what it’s like to be an executive in a company where you’re running everything, like Steve Jobs making all the design decisions for that computer, but of course, there are 100 people inventing things for him to choose from. Anyway, McCarthy moved to Stanford after a few years, but Papert appeared magically and again, we were two people who thought very similarly. And then, when Papert got more interested in education and starting the Logo project, one of my former students named Patrick Winston took on the same role and again, we could communicate – in a few words – large amounts and... after a while, I let him be the main runner of the laboratory and he had his own assistants. And working with Winston was wonderful because I could just say a few words and he would understand more than I had said and would change what some students were doing and we would work together on many theories.

And then many years later, another student appeared named Push Singh. And for the first time again in a decade, I was working closely and he was a great programmer and... we had a partnership of developing this emotion machine class of theories. The trouble is, he unexpectedly, suddenly died. And so, right now I’m in a situation where I don’t have a single major collaborator, which is a little bit strange because it takes... if I get an idea, it usually takes too long to explain it to a stranger. I just have to... write an essay or something. So, I think I’m sort of hoping that someone will turn up again sometime, and I have a couple of possible candidates who might be good collaborators. So, there’s a downside to being partners. If one of them disappears you have to find another one.

My relation to Feynman was very much like the relation with Shannon, namely, we shared the same interest in... in gadgets and ideas so... right, we never actually worked on the same thing, but it would be fantastic to have a conversation with him and come away buzzing with... oh, how come I never thought about that, and why didn’t... I don't know what to say. I mean the number of hours we spent together really wasn’t very large, but every... every encounter would... would leave a series of echoes and... again, if I ever thought about a physics question, then I might say: 'What would Feynman say about this?' But, of course, because of those wonderful books that his friends put together from his lectures, you also could usually find... find... what did Feynman say about this? So, they’re these tapes and records and... what’s the name of the...?
[Q] The Feynman Lectures. 
The Feynman Lectures. And I’d written a paper about a theory of discrete physics, which was partly based on some of the ideas about cellular automata that Fredkin had. And, Feynman liked those a lot. And so, that’s one chapter in one... one of the books of essays. So, we talked about that a little bit. And it always ended up saying... he would be saying: 'I really should be working on cellular automata... because I don’t believe you can pack an infinite amount of information into a finite amount of space. There’s something... something a little bit wrong with modern physics.' But he said: 'First I have to fix quantum chromodynamics, and I’ve almost got it.'

Right now, it’s hard to clearly imagine the future because of the current crisis in climate predictions. 'Cause it looks like in the next 50 years we will pass some critical point and the oceans will rise and the weather will get worse and the food supply will diminish and all sorts of problems like that are facing us. And I would like to see basic research increased so that we can under... make better predictions about these things and fix them, but it’s hard to be very optimistic about it.

One problem that we... I hope we’ll have to face is that people can continue to live longer and longer. Let’s look at the longevity situation at the moment. And it’s pretty interesting because in the last 60 years or so – since penicillin and things like that, which go back to the 1940s – the average lifespan of a person in the so-called developed countries has been increasing roughly at one year every four – or three months per year, and so forth – so, if you take the 60 years since 1950, people are generally living 15 years longer. Which doesn’t seem like very much, but it’s the difference between 75 and 90. That sort of life expectancy. And that ought to keep increasing as medical discoveries are made. And there’s a big difference in the range of predictions about that. So that some optimists predict that there will be huge increases. Other... pessimists think that we may be reaching the limit of the easily controlled diseases and degenerative conditions and the fact is that virtually no one has lived past the age of 120. Just two or three people in recorded history have gotten to 123. And that doesn’t seem to be changing, but of course the records aren’t very good and one doesn’t know. But I think it will change and that longevity probably will grow and that’s very nice.

One problem is we have to maintain the world population at lower levels. And so, imagine a future – maybe 50 years from now – when the average person can expect a lifetime of... say 150 years, but since the population has to be... has to stop growing, that means that for each person of age 150, there will be only about one youngster of age 75. And there'll be virtually no really young people to do the work that right now in developed countries is often done by immigrants. So that rather than facing the problem, we’re just raising a new one. Anyway, one pessimistic view of this is that if we do solve the population problem and get the world population down to a couple of billion, by that time most of them will be rather old and there won’t be young people to do the work. So, we’ll need pretty smart robots. The gloomy part is that a lot of these older people will be disabled and will need nursing robots but, I don’t feel like talking or even thinking about that much, but I think it’s pretty urgent because there aren’t many nurses being trained now. And the number of intellectually still healthy people who are physically partly disabled is going to increase also. So, we’ll have lots of people who are good thinkers who can’t get around much. And so, I’m hoping that there will be a spurt in the level of artificial intelligence to produce inexpensive and resourceful nursing robots and general assistants so that the developed civilisations can maintain their standards without having excessive immigration from other cultures. It’s not that I think any culture is particularly better than another, but it’s not clear that... we want to have everything to get too homogeneous.

Anyway, it does concern me that... that the opportunities for research in my field are not as good as they were when I was a youngster. It was wonderful that if I asked George Miller: 'How could we get some support to build a learning machine like this one?'  He said, well, he’ll find a way. And a few weeks later, he told me that some money had appeared from some magic source and we could go ahead. It’s much harder now and if a young person wants to work on a new field that’s not very promising yet because there’s... there's a prospect... may be 40 years to start to pay off, it’s almost impossible to get support for that. So we need to... we... we need somehow to develop better long range institutions for... for research. I’d like to see many more people working on artificial intelligence. I’ve been complaining a lot that... what happens is that this field tends to get wrapped up in fads. So that, in the 1950s and 1960s – let’s say the 1960s – my laboratory at MIT and Newell and Simon’s laboratory at Carnegie Mellon University were sort of... and Stanford... there were three main groups. Each of them had about 10 or 15 people and we were very productive. Now, I don’t see very many more places like that. The ones that existed are contracting a little bit, but I see very large numbers of people following – what should I call them? – monotheistic fads. Interesting, there's a... I have an idea for a machine that will be intelligent. It will work by making predictions and doing such and such. That’s a nice idea and... there’s a group trying to make that kind of machine. There are many people who say, all we need is a set of perhaps 10,000 or 10 million rules and...  in each situation, the machine has to find which rule will apply to that and make the situation better. And so, in the 1980s there was a great fad in making rule-based intelligent machines. And those turned out to be quite good at solving some problems like airline reservations and – in some cases – even stock market predictions. But of course, that only works for a while until the other people get the same program.

Now, what I want is an anti-fad fad. And if you look at my book The Society of Mind or the later one called The Emotion Machine, the idea there is that it’s nice to find a simple theory of how to make a machine smarter and there are a lot of those. What... the trouble is that none of those simple theories can solve very wide classes of problems. Each kind of learning machine or conditioning machine is good for solving some problems. Each prediction machine is good for solving other problems. And a system that simulates Darwinian evolution... trial and error and copying and breeding and so forth... evolutionary machines can solve other problems, but they don’t have the billions of years that Darwinian evolution had to... to develop human brains. So, what I’m looking for is just to have a few more people working on theories of how to combine other theories. It seems to me that the secret of the human brain and... why are we so much more resourceful than all the other animals put together?... is because we... we don’t work according to some particular clever... clever way of thinking. We’re not purely logical. We’re not purely memory based. We’re not purely rule based. But we have maybe 20 or 30 different strategies for learning and thinking and problem solving. And we have higher level management... we’re like committees of competing interests and competing ways to do things. And so, I’d like to see more people funded so that they can spend five or 10 years trying to develop these higher level, more complicated theories. There's just... and I see that this is very important for the future because when we have three or four billion older people and very few younger people, we’ll need these smarter machines to... to facilitate what they do in their everyday lives and... robotic vacuum cleaners are already pretty good, but there’s no machine that can make a bed and there’s... there's just so many things that every normal person can do. I don’t think any robot has ever put a pillow into a pillowcase. It would be fun to try, but I... I think probably everyone would lose that contest today.
